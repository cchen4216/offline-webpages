<!DOCTYPE html>
<!-- saved from url=(0151)https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd -->
<html xmlns:cc="http://creativecommons.org/ns#" class=""><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors</title><link rel="canonical" href="https://gist.github.com/Yorko/f277786fab129d97a22a9ce795bf6fe6"><meta name="title" content="Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors"><meta name="referrer" content="always"><meta name="description" content="Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting! The following material is better viewed as a Jupyter notebook and can be reproduced…"><meta name="theme-color" content="#000000"><meta property="og:title" content="Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors"><meta property="og:url" content="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*5Tt3NDpNobfU_YeKglBRyQ.jpeg"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!"><meta name="twitter:description" content="Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!"><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*5Tt3NDpNobfU_YeKglBRyQ.jpeg"><link rel="publisher" href="https://plus.google.com/103654360130207659246"><link rel="author" href="https://medium.com/@yurykashnitskiy"><meta property="author" content="Yury Kashnitskiy"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="Yury Kashnitskiy"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2018-02-19T09:45:25.404Z"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="28 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/8613c6b6d2cd"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/8613c6b6d2cd"><meta property="al:android:url" content="medium://p/8613c6b6d2cd"><meta property="al:web:url" content="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/8613c6b6d2cd"><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1031,"height":775,"url":"https://cdn-images-1.medium.com/max/2000/1*5Tt3NDpNobfU_YeKglBRyQ.jpeg"},"datePublished":"2018-02-19T09:45:25.404Z","dateModified":"2018-05-07T20:43:47.281Z","headline":"Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors","name":"Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors","keywords":["Machine Learning","Classification","Decision Tree","Scikit Learn","Education"],"author":{"@type":"Person","name":"Yury Kashnitskiy","url":"https://medium.com/@yurykashnitskiy"},"creator":["Yury Kashnitskiy"],"publisher":{"@type":"Organization","name":"Open Machine Learning Course","url":"https://medium.com/open-machine-learning-course","logo":{"@type":"ImageObject","width":107,"height":60,"url":"https://cdn-images-1.medium.com/max/214/1*MxsqShnPs3RXMScwl5HhsQ.jpeg"}},"mainEntityOfPage":"https://gist.github.com/Yorko/f277786fab129d97a22a9ce795bf6fe6"}</script><link rel="stylesheet" type="text/css" id="glyph-8" href="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/m2.css"><link rel="stylesheet" href="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/main-branding-base.cbTc2bTSptidNvlM50PkCQ.css"><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async="" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/analytics.js"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/304/304/1*b5pfoCQz20rjh79OPSQWQg.jpeg"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/240/240/1*b5pfoCQz20rjh79OPSQWQg.jpeg"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/152/152/1*b5pfoCQz20rjh79OPSQWQg.jpeg"><link rel="apple-touch-icon" sizes="60x60" href="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_b5pfoCQz20rjh79OPSQWQg.jpeg"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope="" class="postShowScreen browser-chrome os-mac is-withMagicUnderlinesv-glyph v-glyph--m2 is-js is-withMagicUnderlines" data-action-scope="_actionscope_0"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main surface-container" id="container"><div class="butterBar butterBar--error" data-action-scope="_actionscope_1"></div><div class="surface" id="_obv.shell._surface_1525760897331" style="display: block; visibility: visible;"><div class="screenContent surface-content is-supplementalPostContentLoaded" data-used="true" data-action-scope="_actionscope_2"><canvas class="canvas-renderer metabar-sibling" width="1680" height="932"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer metabar-sibling"><div class="notesMarkers" data-action-scope="_actionscope_4"></div></div><div class="metabar u-clearfix js-metabar is-hiddenWhenMinimized metabar--affixed is-maximized"><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1000 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingLeft20 u-paddingRight20"><div class="metabar-block u-flex1  u-flexCenter"><div class="js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-textColorDarker"><svg class="svgIcon-use" width="45" height="45" viewBox="0 0 45 45"><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"></path></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56"><div class="u-alignBlock"><span class="u-inlineBlock u-height28 u-xs-height24 u-verticalAlignTop u-marginRight20 u-marginLeft15 u-borderRightLighter"></span></div></div><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56 u-marginRight18"><div class="u-alignBlock"><a class="js-collectionLogoOrName" href="https://medium.com/open-machine-learning-course?source=logo-a988255767e5---b76595fb6cfc"><img height="36" width="65" class="u-paddingTop5" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_MxsqShnPs3RXMScwl5HhsQ.jpeg" alt="Open Machine Learning Course"></a></div></div><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56 u-xs-hide"><div class="u-alignBlock"><div class="buttonSet"><button class="button button--primary button--filled button--smallest button--withChrome u-accentColor--buttonNormal button--withIcon button--withSvgIcon button--withIconRight button--withIconAndLabel js-relationshipButton is-smallPill" data-action="show-more-collection-actions" data-action-value="affixMenu" data-collection-id="b76595fb6cfc"><span class="button-label  js-buttonLabel">Following</span><span class="svgIcon svgIcon--arrowDown svgIcon--11px u-relative u-top1 u-paddingLeft6"><svg class="svgIcon-use" width="11" height="11" viewBox="0 0 11 11"><path d="M0 2.77l5.095 5.648.427.472.427-.472L10.99 2.83l-.85-.77-4.618 5.142L.85 2z" fill-rule="evenodd"></path></svg></span></button><a class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon" href="https://twitter.com/odsai_en" title="Visit “Open Machine Learning Course” on Twitter" aria-label="Visit “Open Machine Learning Course” on Twitter" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></span></a><a class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-paddingLeft0" href="https://facebook.com/DataChallenges" title="Visit “Open Machine Learning Course” on Facebook" aria-label="Visit “Open Machine Learning Course” on Facebook" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M21 12.646C21 7.65 16.97 3.6 12 3.6s-9 4.05-9 9.046a9.026 9.026 0 0 0 7.59 8.924v-6.376H8.395V12.64h2.193v-1.88c0-2.186 1.328-3.375 3.267-3.375.93 0 1.728.07 1.96.1V9.77H14.47c-1.055 0-1.26.503-1.26 1.242v1.63h2.517l-.33 2.554H13.21V21.6c4.398-.597 7.79-4.373 7.79-8.954"></path></svg></span></span></a></div></div></div></div><div class="metabar-block u-flex0 u-flexCenter"><div class="u-alignMiddle u-inlineBlock u-verticalAlignTop u-height65 u-xs-height56"><div class="u-alignBlock"><div class="buttonSet buttonSet--wide u-lineHeightInherit"><label class="button button--chromeless inputGroup u-sm-hide metabar-predictiveSearch u-baseColor--buttonNormal u-baseColor--placeholderNormal" title="Search"><span class="svgIcon svgIcon--search svgIcon--25px u-top0 u-baseColor--iconLight"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span><input class="js-predictiveSearchInput textInput textInput--rounded textInput--darkText u-baseColor--textNormal textInput--transparent" type="search" placeholder="Search" required="true" data-collection-id="b76595fb6cfc"></label><a class="button button--small button--chromeless u-sm-show is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-xs-top2" href="https://medium.com/open-machine-learning-course/search" title="Search" aria-label="Search"><span class="button-defaultState"><span class="svgIcon svgIcon--search svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M20.067 18.933l-4.157-4.157a6 6 0 1 0-.884.884l4.157 4.157a.624.624 0 1 0 .884-.884zM6.5 11c0-2.62 2.13-4.75 4.75-4.75S16 8.38 16 11s-2.13 4.75-4.75 4.75S6.5 13.62 6.5 11z"></path></svg></span></span></a><button class="button button--small button--chromeless is-inSiteNavBar u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--activity js-notificationsButton u-marginRight16 u-xs-marginRight5 u-lineHeight0 u-size25x25" title="Notifications" aria-label="Notifications" data-action="open-notifications"><span class="svgIcon svgIcon--bell svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-293 409 25 25"><path d="M-273.327 423.67l-1.673-1.52v-3.646a5.5 5.5 0 0 0-6.04-5.474c-2.86.273-4.96 2.838-4.96 5.71v3.41l-1.68 1.553c-.204.19-.32.456-.32.734V427a1 1 0 0 0 1 1h3.49a3.079 3.079 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59c0-.28-.12-.55-.327-.74zm-7.173 5.63c-.842 0-1.55-.546-1.812-1.3h3.624a1.92 1.92 0 0 1-1.812 1.3zm6.35-2.45h-12.7v-2.347l1.63-1.51c.236-.216.37-.522.37-.843v-3.41c0-2.35 1.72-4.356 3.92-4.565a4.353 4.353 0 0 1 4.78 4.33v3.645c0 .324.137.633.376.85l1.624 1.477v2.373z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal u-marginLeft4 is-inSiteNavBar js-userActions is-touched" aria-haspopup="true" data-action="open-userActions"><div class="avatar"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_P-t7h_lDOjJiWVxI" class="avatar-image avatar-image--icon" alt="陈辰"></div></button></div></div></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56 metabar-sibling"></div><main role="main" class="metabar-sibling"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full is-withAccentColors" lang="en"><header class="container u-maxWidth740"><div class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaHeader u-paddingBottom10 row"><div class="col u-size12of12 js-postMetaLockup"><div class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaLockup postMetaLockup--authorWithBio u-flexCenter js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@yurykashnitskiy?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="5572ec600139" data-action-type="hover" data-user-id="5572ec600139" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_JtleyoApNKIjJU5F" class="avatar-image avatar-image--small" alt="Go to the profile of Yury Kashnitskiy"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-lineHeightTightest"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker" href="https://medium.com/@yurykashnitskiy?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="5572ec600139" data-action-type="hover" data-user-id="5572ec600139" dir="auto">Yury Kashnitskiy</a><span class="followState js-followState" data-user-id="5572ec600139"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide" data-action="toggle-block-user" data-action-value="5572ec600139" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-marginLeft10 u-xs-hide" data-action="toggle-subscribe-user" data-action-value="5572ec600139" data-action-source="post_header_lockup-5572ec600139-------------------------follow_byline" data-subscribe-source="post_header_lockup" data-follow-context-entity-id="8613c6b6d2cd"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption ui-xs-clamp2 postMetaInline">Data Scientist at Mail.Ru Group</div><div class="ui-caption postMetaInline js-testPostMetaInlineSupplemental"><time datetime="2018-02-19T09:45:25.404Z">Feb 19</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="28 min read"></span></div></div></div></div></div></header><div class="postArticle-content js-postField js-notesSource js-trackedPost" data-post-id="8613c6b6d2cd" data-source="post_page" data-collection-id="b76595fb6cfc" data-tracking-context="postPage" data-scroll="native"><section name="47bc" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="7b0c" id="7b0c" class="graf graf--h3 graf--leading graf--title">Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors</h1><figure name="83af" id="83af" class="graf graf--figure graf-after--h3"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 526px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*5Tt3NDpNobfU_YeKglBRyQ.jpeg" data-width="1031" data-height="775" data-is-featured="true" data-action="zoom" data-action-value="1*5Tt3NDpNobfU_YeKglBRyQ.jpeg" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_5Tt3NDpNobfU_YeKglBRyQ.jpeg" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="56"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*5Tt3NDpNobfU_YeKglBRyQ.jpeg" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_5Tt3NDpNobfU_YeKglBRyQ(1).jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*5Tt3NDpNobfU_YeKglBRyQ.jpeg"></noscript></div></div><figcaption class="imageCaption">A fractal tree.&nbsp;<a href="https://alvenka.deviantart.com/art/fractal-tree-34-366989431" data-href="https://alvenka.deviantart.com/art/fractal-tree-34-366989431" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Source</a></figcaption></figure><p name="aea6" id="aea6" class="graf graf--p graf-after--figure">Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!</p><h3 name="c3b5" id="c3b5" class="graf graf--h3 graf-after--p">Article outline</h3><ol class="postList"><li name="96e1" id="96e1" class="graf graf--li graf-after--h3">Introduction</li><li name="462e" id="462e" class="graf graf--li graf-after--li">Decision Tree</li></ol><ul class="postList"><li name="2063" id="2063" class="graf graf--li graf-after--li">How to Build a Decision Tree</li><li name="0e94" id="0e94" class="graf graf--li graf-after--li">Tree-building Algorithm</li><li name="740f" id="740f" class="graf graf--li graf-after--li">Other Quality Criteria for Splits in Classification Problems</li><li name="de8f" id="de8f" class="graf graf--li graf-after--li">How a Decision Tree Works with Numerical Features</li><li name="9f91" id="9f91" class="graf graf--li graf-after--li">Crucial Tree Parameters</li><li name="89a8" id="89a8" class="graf graf--li graf-after--li">Class <code class="markup--code markup--li-code">DecisionTreeClassifier</code> in Scikit-learn</li><li name="22e0" id="22e0" class="graf graf--li graf-after--li">Decision Tree in a Regression Problem</li></ul><p name="1d31" id="1d31" class="graf graf--p graf-after--li">3. Nearest Neighbors Method</p><ul class="postList"><li name="5c64" id="5c64" class="graf graf--li graf-after--p">Nearest Neighbors Method in Real Applications</li><li name="3af0" id="3af0" class="graf graf--li graf-after--li">Class <code class="markup--code markup--li-code">KNeighborsClassifier</code> in Scikit-learn</li></ul><p name="6e0b" id="6e0b" class="graf graf--p graf-after--li">4. Choosing Model Parameters and Cross-Validation</p><p name="933e" id="933e" class="graf graf--p graf-after--p">5. Application Examples and Complex Cases</p><ul class="postList"><li name="c0d7" id="c0d7" class="graf graf--li graf-after--p">Decision trees and nearest neighbors method in a customer churn prediction task</li><li name="a127" id="a127" class="graf graf--li graf-after--li">Complex Case for Decision Trees</li><li name="e454" id="e454" class="graf graf--li graf-after--li">Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition</li><li name="5757" id="5757" class="graf graf--li graf-after--li">Complex Case for the Nearest Neighbors Method</li></ul><p name="d290" id="d290" class="graf graf--p graf-after--li">6. Pros and Cons of Decision Trees and the Nearest Neighbors Method</p><p name="a033" id="a033" class="graf graf--p graf-after--p">7. Assignment #3</p><p name="617e" id="617e" class="graf graf--p graf-after--p">8. Useful resources</p><p name="11b6" id="11b6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The following material is better viewed as a </strong><a href="http://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb?flush_cache=true" data-href="http://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb?flush_cache=true" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Jupyter notebook</strong></a><strong class="markup--strong markup--p-strong"> and can be reproduced locally with Jupyter if you clone the </strong><a href="https://github.com/Yorko/mlcourse_open" data-href="https://github.com/Yorko/mlcourse_open" class="markup--anchor markup--p-anchor" rel="noopener nofollow nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">course repository</strong></a><strong class="markup--strong markup--p-strong">.</strong></p><h3 name="43ed" id="43ed" class="graf graf--h3 graf-after--p">1. Introduction</h3><p name="1041" id="1041" class="graf graf--p graf-after--h3">Before we dive into the material for this week’s article, let’s talk about the kind of problem that we are going to solve and its place in the exciting field of machine learning. T. Mitchell’s book <em class="markup--em markup--p-em">Machine Learning</em> (1997) gives a classic, general definition of machine learning as follows:</p><blockquote name="7873" id="7873" class="graf graf--blockquote graf-after--p">A computer program is said to learn from experience <em class="markup--em markup--blockquote-em">E</em> with respect to some class of tasks <em class="markup--em markup--blockquote-em">T</em> and performance measure <em class="markup--em markup--blockquote-em">P</em>, if its performance at tasks in <em class="markup--em markup--blockquote-em">T</em>, as measured by <em class="markup--em markup--blockquote-em">P</em>, improves with experience <em class="markup--em markup--blockquote-em">E</em>.</blockquote><p name="2bfc" id="2bfc" class="graf graf--p graf-after--blockquote">In the various problem settings <em class="markup--em markup--p-em">T</em>, <em class="markup--em markup--p-em">P</em>, and <em class="markup--em markup--p-em">E</em> can refer to completely different things. Some of the most popular <strong class="markup--strong markup--p-strong">tasks <em class="markup--em markup--p-em">T</em> in machine learning</strong> are the following:</p><ul class="postList"><li name="d5b9" id="d5b9" class="graf graf--li graf-after--p">classification of an instance to one of the categories based on its features;</li><li name="7c98" id="7c98" class="graf graf--li graf-after--li">regression — prediction of a numerical target feature based on other features of an instance;</li><li name="e577" id="e577" class="graf graf--li graf-after--li">clustering — identifying partitions of instances based on the features of these instances so that the members within the groups are more similar to each other than those in the other groups;</li><li name="bb0b" id="bb0b" class="graf graf--li graf-after--li">anomaly detection — search for instances that are “greatly dissimilar” to the rest of the sample or to some group of instances;</li><li name="b3df" id="b3df" class="graf graf--li graf-after--li">and so many more.</li></ul><p name="ca89" id="ca89" class="graf graf--p graf-after--li">A good overview is provided in the “Machine Learning basics” chapter of <a href="http://www.deeplearningbook.org/" data-href="http://www.deeplearningbook.org/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">“Deep Learning”</a> (by Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).</p><p name="c5c7" id="c5c7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Experience <em class="markup--em markup--p-em">E</em></strong> refers to data (we can’t go anywhere without it). Machine learning algorithms can be divided into those that are trained in <em class="markup--em markup--p-em">supervised</em> or <em class="markup--em markup--p-em">unsupervised</em> manner. In unsupervised learning tasks, one has a <em class="markup--em markup--p-em">set</em> consisting of <em class="markup--em markup--p-em">instances</em> described by a set of <em class="markup--em markup--p-em">features</em>. In supervised learning problems, there’s also a <em class="markup--em markup--p-em">target variable</em>, which is what we would like to be able to predict, known for each instance in a <em class="markup--em markup--p-em">training set</em>.</p><h4 name="9896" id="9896" class="graf graf--h4 graf-after--p">Example</h4><p name="26c6" id="26c6" class="graf graf--p graf-after--h4">Classification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, the experience <em class="markup--em markup--p-em">E</em> is the available training data: a set of <em class="markup--em markup--p-em">instances</em> (clients), a collection of <em class="markup--em markup--p-em">features</em> (such as age, salary, type of loan, past loan defaults, etc.) for each, and a <em class="markup--em markup--p-em">target variable</em> (whether they defaulted on the loan). This target variable is just a fact of loan default (1 or 0), so recall that this is a (binary) classification problem. If you were instead predicting <em class="markup--em markup--p-em">by how much time</em> the loan payment is overdue, this would become a regression problem.</p><p name="13bc" id="13bc" class="graf graf--p graf-after--p">Finally, the third term used in the definition of machine learning is a <strong class="markup--strong markup--p-strong">metric of the algorithm’s performance evaluation <em class="markup--em markup--p-em">P</em>.</strong> Such metrics differ for various problems and algorithms, and we’ll discuss them as we study new algorithms. For now, we’ll refer to a simple metric for classification algorithms, the proportion of correct answers — <em class="markup--em markup--p-em">accuracy</em> — on the test set.</p><p name="91f4" id="91f4" class="graf graf--p graf-after--p">Let’s take a look at two supervised learning problems: classification and regression.</p><h3 name="13a9" id="13a9" class="graf graf--h3 graf-after--p">2. Decision&nbsp;Tree</h3><p name="a33b" id="a33b" class="graf graf--p graf-after--h3">We begin our overview of classification and regression methods with one of the most popular ones — a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal.</p><figure name="e68d" id="e68d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 518px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 74%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*CIb1qGHEY-UptAWD.png" data-width="1498" data-height="1108" data-action="zoom" data-action-value="0*CIb1qGHEY-UptAWD.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_CIb1qGHEY-UptAWD.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="55"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/0*CIb1qGHEY-UptAWD.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_CIb1qGHEY-UptAWD(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*CIb1qGHEY-UptAWD.png"></noscript></div></div></figure><p name="132d" id="132d" class="graf graf--p graf-after--figure">In terms of machine learning, one can see it as a simple classifier that determines the appropriate form of publication (book, article, chapter of the book, preprint, publication in the “Higher School of Economics and the Media”) based on the content (book, pamphlet, paper), type of journal, original publication type (scientific journal, proceedings), etc.</p><p name="446a" id="446a" class="graf graf--p graf-after--p">A decision tree is often a generalization of the experts’ experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively (or empirically) derived rules that could be represented as a decision tree.</p><figure name="8d8e" id="8d8e" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 440px; max-height: 298px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 67.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*gaIruEZ7kHJeLz0bhUo_GQ.png" data-width="440" data-height="298" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_gaIruEZ7kHJeLz0bhUo_GQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*gaIruEZ7kHJeLz0bhUo_GQ.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_gaIruEZ7kHJeLz0bhUo_GQ(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*gaIruEZ7kHJeLz0bhUo_GQ.png"></noscript></div></div></figure><p name="9101" id="9101" class="graf graf--p graf-after--figure">In our next case, we solve a binary classification problem (approve/deny a loan) on the grounds of “Age”, “Home-ownership”, “Income” and “Education”.</p><p name="d442" id="d442" class="graf graf--p graf-after--p">The decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form “feature <em class="markup--em markup--p-em">a</em> value is less than <em class="markup--em markup--p-em">x </em>and feature <em class="markup--em markup--p-em">b</em> value is less than <em class="markup--em markup--p-em">y</em>&nbsp;… =&gt; Category 1” into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000.</p><p name="64da" id="64da" class="graf graf--p graf-after--p">As we’ll see later, many other models, although more accurate, do not have this property and can be regarded as more of a “black box” approach, where it is harder to interpret how the input data was transformed into the output. Due to this “understandability” and similarity to human decision-making (you can easily explain your model to your boss), decision trees have gained immense popularity. C4.5, a representative of this group of classification methods, is even the first in the list of 10 best data mining algorithms (“Top 10 Algorithms in Data Mining”, Knowledge and Information Systems, 2008. <a href="http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf" data-href="http://www.cs.uvm.edu/%7Eicdm/algorithms/10Algorithms-08.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">PDF</a>).</p><h3 name="c675" id="c675" class="graf graf--h3 graf-after--p">How to Build a Decision&nbsp;Tree</h3><p name="c9cb" id="c9cb" class="graf graf--p graf-after--h3">Earlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let’s discuss a simple example where all the variables are binary.</p><p name="5536" id="5536" class="graf graf--p graf-after--p">Recall the game of “20 Questions”, which is often referenced when introducing decision trees. You’ve probably played this game — one person thinks of a celebrity while the other tries to guess by asking only “Yes” or “No” questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking “Is it Angelina Jolie?” would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking “Is the celebrity a woman?” would reduce the possibilities to roughly half. That is to say, the “gender” feature separates the celebrity dataset much better than other features like “Angelina Jolie”, “Spanish”, or “loves football.” This reasoning corresponds to the concept of information gain based on entropy.</p><h4 name="69be" id="69be" class="graf graf--h4 graf-after--p">Entropy</h4><p name="195b" id="195b" class="graf graf--p graf-after--h4">Shannon’s entropy is defined for a system with N possible states as follows:</p><figure name="8325" id="8325" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 353px; max-height: 133px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*cf-lyJLXXEDSK2H7GN8ncw.png" data-width="353" data-height="133" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_cf-lyJLXXEDSK2H7GN8ncw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="27"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*cf-lyJLXXEDSK2H7GN8ncw.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_cf-lyJLXXEDSK2H7GN8ncw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*cf-lyJLXXEDSK2H7GN8ncw.png"></noscript></div></div></figure><p name="afc5" id="afc5" class="graf graf--p graf-after--figure">where <em class="markup--em markup--p-em">Pi </em>is the probability of finding the system in the <em class="markup--em markup--p-em">i</em>-th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. This will help us formalize “effective data splitting”, which we alluded to in the context of “20 Questions”.</p><h4 name="9599" id="9599" class="graf graf--h4 graf-after--p">Toy Example</h4><p name="597c" id="597c" class="graf graf--p graf-after--h4">To illustrate how entropy can help us identify good features for building a decision tree, let’s look at a toy example. We will predict the color of the ball based on its position.</p><figure name="90e9" id="90e9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 606px; max-height: 96px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.8%;"></div><img class="graf-image" data-image-id="0*jW6I6V_I-Nn725It.png" data-width="606" data-height="96" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_jW6I6V_I-Nn725It.png"></div><figcaption class="imageCaption"><a href="https://habrahabr.ru/post/171759/" data-href="https://habrahabr.ru/post/171759/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Source</a> (in&nbsp;Russian)</figcaption></figure><p name="2033" id="2033" class="graf graf--p graf-after--figure">There are 9 blue balls and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability p1 = 9/20 and yellow with probability p2 = 11/20, which gives us an entropy S0 = -9/20 log2(9/20) - 11/20 log2(11/20) ≈ 1. This value by itself may not tell us much, but let’s see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12.</p><figure name="72b9" id="72b9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 215px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*JSFOJNkdOHNhSjdo.png" data-width="717" data-height="220" data-action="zoom" data-action-value="0*JSFOJNkdOHNhSjdo.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_JSFOJNkdOHNhSjdo.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="22"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/0*JSFOJNkdOHNhSjdo.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_JSFOJNkdOHNhSjdo(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*JSFOJNkdOHNhSjdo.png"></noscript></div></div><figcaption class="imageCaption"><a href="https://habrahabr.ru/post/171759/" data-href="https://habrahabr.ru/post/171759/" class="markup--anchor markup--figure-anchor" rel="noopener nofollow" target="_blank">Source</a> (in&nbsp;Russian)</figcaption></figure><p name="a28a" id="a28a" class="graf graf--p graf-after--figure">The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is S1 = -5/13 log2(5/13) - 8/13 log2(8/13) ≈ 0.96. The right group has 7 balls, 1 blue and 6 yellow. The entropy of the right group is S2 = -1/7 log2(1/7) - 6/7 log2(6/7) ≈ 0.6. As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos (or uncertainty) in the system, the reduction in entropy is called information gain. Formally, the information gain (IG) for a split based on the variable <em class="markup--em markup--p-em">Q</em>(in this example it’s a variable “<em class="markup--em markup--p-em">x ≤ 12</em>”) is defined as</p><figure name="023b" id="023b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 429px; max-height: 132px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*ewOFVFQEXRwamSPBwY2QGw.png" data-width="429" data-height="132" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_ewOFVFQEXRwamSPBwY2QGw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="22"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*ewOFVFQEXRwamSPBwY2QGw.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_ewOFVFQEXRwamSPBwY2QGw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*ewOFVFQEXRwamSPBwY2QGw.png"></noscript></div></div></figure><p name="0b84" id="0b84" class="graf graf--p graf-after--figure">where <em class="markup--em markup--p-em">q </em>is the number of groups after the split, <em class="markup--em markup--p-em">Ni</em> is number of objects from the sample in which variable <em class="markup--em markup--p-em">Q</em> is equal to the <em class="markup--em markup--p-em">i</em>-th value. In our example, our split yielded two groups (q = 2), one with 13 elements (N1 = 13), the other with 7 (N2 = 7). Therefore, we can compute the information gain as</p><figure name="ca5d" id="ca5d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 97px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 13.8%;"></div><img class="graf-image" data-image-id="1*wNF6UPLh_H-J0u70tlQU1g.png" data-width="703" data-height="97" data-action="zoom" data-action-value="1*wNF6UPLh_H-J0u70tlQU1g.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_wNF6UPLh_H-J0u70tlQU1g.png"></div></figure><p name="12b7" id="12b7" class="graf graf--p graf-after--figure">It turns out that dividing the balls into two groups by splitting on “coordinate is less than or equal to 12” gave us a more ordered system. Let’s continue to divide them into groups until the balls in each group are all of the same color.</p><p name="2816" id="2816" class="graf graf--p graf-after--p">For the right group, we can easily see that we only need one extra partition using “coordinate less than or equal to 18”. But, for the left group, we need three more. Note that the entropy of a group where all of the balls are the same color is equal to 0 (log2(1) = 0).</p><p name="5466" id="5466" class="graf graf--p graf-after--p">We have successfully constructed a decision tree that predicts ball color based on its position. This decision tree may not work well if we add any balls because it has perfectly fit to the training set (initial 20 balls). If we wanted to do well in that case, a tree with fewer “questions” or splits would be more accurate, even if it does not perfectly fit the training set. We will discuss the problem of overfitting later.</p><h3 name="07cc" id="07cc" class="graf graf--h3 graf-after--p">Tree-building Algorithm</h3><p name="028f" id="028f" class="graf graf--p graf-after--h3">We can make sure that the tree built in the previous example is optimal: it took only 5 “questions” (conditioned on the variable <em class="markup--em markup--p-em">x</em>) to perfectly fit a decision tree to the training set. Under other split conditions, the resulting tree would be deeper, i.e. take more “questions” to reach an answer.</p><p name="75c6" id="75c6" class="graf graf--p graf-after--p">At the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for “early stopping” or “cut-off” to avoid constructing an overfitted tree.</p><figure name="56e4" id="56e4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 269px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*cKuUJ3ARs8tTk15Of_4btg.png" data-width="718" data-height="276" data-action="zoom" data-action-value="1*cKuUJ3ARs8tTk15Of_4btg.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_cKuUJ3ARs8tTk15Of_4btg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="28"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*cKuUJ3ARs8tTk15Of_4btg.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_cKuUJ3ARs8tTk15Of_4btg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*cKuUJ3ARs8tTk15Of_4btg.png"></noscript></div></div></figure><h3 name="1ac6" id="1ac6" class="graf graf--h3 graf-after--figure">Other Quality Criteria for Splits in Classification Problems</h3><p name="34df" id="34df" class="graf graf--p graf-after--h3">We discussed how entropy allows us to formalize partitions in a tree. But this is only one heuristic; there exist others.</p><figure name="1986" id="1986" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 297px; max-height: 106px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*tQO9YrY-ntGAjM_RzW-pkA.png" data-width="297" data-height="106" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_tQO9YrY-ntGAjM_RzW-pkA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="26"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*tQO9YrY-ntGAjM_RzW-pkA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_tQO9YrY-ntGAjM_RzW-pkA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*tQO9YrY-ntGAjM_RzW-pkA.png"></noscript></div></div><figcaption class="imageCaption">Gini uncertainty (Gini impurity)</figcaption></figure><p name="1efd" id="1efd" class="graf graf--p graf-after--figure">Maximizing this criterion can be interpreted as the maximization of the number of pairs of objects of the same class that are in the same subtree (not to be confused with the Gini index).</p><figure name="b1e5" id="b1e5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 271px; max-height: 72px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 26.6%;"></div><img class="graf-image" data-image-id="1*WOB8ZXlkfv-DLHvzaZl88A.png" data-width="271" data-height="72" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_WOB8ZXlkfv-DLHvzaZl88A.png"></div><figcaption class="imageCaption">Misclassification error</figcaption></figure><p name="6d89" id="6d89" class="graf graf--p graf-after--figure">In practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.</p><p name="5e8c" id="5e8c" class="graf graf--p graf-after--p">For binary classification, entropy and Gini uncertainty take the following form:</p><figure name="004b" id="004b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 80px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 11.4%;"></div><img class="graf-image" data-image-id="1*ijHoiJ08hV0KckUbIMERcw.png" data-width="738" data-height="84" data-action="zoom" data-action-value="1*ijHoiJ08hV0KckUbIMERcw.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_ijHoiJ08hV0KckUbIMERcw.png"></div></figure><p name="ba46" id="ba46" class="graf graf--p graf-after--figure">where (<em class="markup--em markup--p-em">p+</em> is the probability of an object having a label +).</p><p name="328b" id="328b" class="graf graf--p graf-after--p">If we plot these two functions against the argument <em class="markup--em markup--p-em">p+</em>, we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical.</p><figure name="17d9" id="17d9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 467px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*GC23qi3fl2hrWW4eUComgw.png" data-width="1800" data-height="1200" data-action="zoom" data-action-value="1*GC23qi3fl2hrWW4eUComgw.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_GC23qi3fl2hrWW4eUComgw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="50"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*GC23qi3fl2hrWW4eUComgw.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_GC23qi3fl2hrWW4eUComgw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*GC23qi3fl2hrWW4eUComgw.png"></noscript></div></div></figure><h4 name="72af" id="72af" class="graf graf--h4 graf-after--figure">Example</h4><p name="41c9" id="41c9" class="graf graf--p graf-after--h4">Let’s consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means.</p><figure name="77b3" id="77b3" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/2ef293179efea50d277703e91e4768df?postId=8613c6b6d2cd" data-media-id="2ef293179efea50d277703e91e4768df" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/2ef293179efea50d277703e91e4768df.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/2ef293179efea50d277703e91e4768df?postId=8613c6b6d2cd" data-media-id="2ef293179efea50d277703e91e4768df" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="c6b9" id="c6b9" class="graf graf--p graf-after--figure">Let’s plot the data. Informally, the classification problem in this case is to build some “good” boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or, at least, a straight line or a hyperplane, would work well on new data.</p><figure name="627a" id="627a" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.714%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/fc55ac1553e94cc62ff5b94fc3b0b6c2?postId=8613c6b6d2cd" data-media-id="fc55ac1553e94cc62ff5b94fc3b0b6c2" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/fc55ac1553e94cc62ff5b94fc3b0b6c2.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/fc55ac1553e94cc62ff5b94fc3b0b6c2?postId=8613c6b6d2cd" data-media-id="fc55ac1553e94cc62ff5b94fc3b0b6c2" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="989e" id="989e" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 560px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*77R2NOZymudraI89GH9LuA.png" data-width="3000" data-height="2400" data-action="zoom" data-action-value="1*77R2NOZymudraI89GH9LuA.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_77R2NOZymudraI89GH9LuA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="60"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*77R2NOZymudraI89GH9LuA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_77R2NOZymudraI89GH9LuA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*77R2NOZymudraI89GH9LuA.png"></noscript></div></div></figure><p name="6d46" id="6d46" class="graf graf--p graf-after--figure">Let’s try to separate these two classes by training an <code class="markup--code markup--p-code">Sklearn</code> decision tree. We will use <code class="markup--code markup--p-code">max_depth</code> parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.</p><figure name="5ab5" id="5ab5" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 72.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/f274a20bcb6b98c87a68863e5dd4c043?postId=8613c6b6d2cd" data-media-id="f274a20bcb6b98c87a68863e5dd4c043" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/f274a20bcb6b98c87a68863e5dd4c043.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/f274a20bcb6b98c87a68863e5dd4c043?postId=8613c6b6d2cd" data-media-id="f274a20bcb6b98c87a68863e5dd4c043" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="ed2b" id="ed2b" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 560px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*4t4IrcQdKplQsAE4SBduXg.png" data-width="3000" data-height="2400" data-action="zoom" data-action-value="1*4t4IrcQdKplQsAE4SBduXg.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_4t4IrcQdKplQsAE4SBduXg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="60"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*4t4IrcQdKplQsAE4SBduXg.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_4t4IrcQdKplQsAE4SBduXg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*4t4IrcQdKplQsAE4SBduXg.png"></noscript></div></div></figure><p name="6703" id="6703" class="graf graf--p graf-after--figure">And how does the tree itself look? We see that the tree “cuts” the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it.</p><figure name="39ff" id="39ff" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.571%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/664fe19fd7f66fcee668fa5042fea92f?postId=8613c6b6d2cd" data-media-id="664fe19fd7f66fcee668fa5042fea92f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/664fe19fd7f66fcee668fa5042fea92f.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/664fe19fd7f66fcee668fa5042fea92f?postId=8613c6b6d2cd" data-media-id="664fe19fd7f66fcee668fa5042fea92f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure></div><div class="section-inner sectionLayout--outsetColumn"><figure name="d6f7" id="d6f7" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 389px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*aNhu3p8AkkGs3uJlKdLUjg.png" data-width="1227" data-height="477" data-action="zoom" data-action-value="1*aNhu3p8AkkGs3uJlKdLUjg.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_aNhu3p8AkkGs3uJlKdLUjg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="28"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*aNhu3p8AkkGs3uJlKdLUjg.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_aNhu3p8AkkGs3uJlKdLUjg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*aNhu3p8AkkGs3uJlKdLUjg.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><h4 name="c0fc" id="c0fc" class="graf graf--h4 graf-after--figure">How can we “read” such a&nbsp;tree?</h4><p name="e23e" id="e23e" class="graf graf--p graf-after--h4">In the beginning, there were 200 samples (instances), 100 of each class. The entropy of the initial state was maximal, S=1. Then, the first partition of the samples into 2 groups was made by comparing the value of <em class="markup--em markup--p-em">x2</em> with 1.211 (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white.</p><h3 name="5262" id="5262" class="graf graf--h3 graf-after--p">How a Decision Tree Works with Numerical Features</h3><p name="15c7" id="15c7" class="graf graf--p graf-after--h3">Suppose we have a numeric feature “Age” that has a lot of unique values. A decision tree will look for the best (according to some criterion of information gain) split by checking binary attributes such as “Age &lt;17”, “Age &lt; 22.87”, and so on. But what if the age range is large? Or what if another quantitative variable, “salary”, can also be “cut” in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.</p><p name="2c79" id="2c79" class="graf graf--p graf-after--p">Let’s consider an example. Suppose we have the following dataset:</p><figure name="bdc4" id="bdc4" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.714%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/dfb2658bcc651357a93115d2ac6bf5ea?postId=8613c6b6d2cd" data-media-id="dfb2658bcc651357a93115d2ac6bf5ea" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/dfb2658bcc651357a93115d2ac6bf5ea.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/dfb2658bcc651357a93115d2ac6bf5ea?postId=8613c6b6d2cd" data-media-id="dfb2658bcc651357a93115d2ac6bf5ea" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="27a8" id="27a8" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 220px; max-height: 489px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 222.29999999999998%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*d17quX4zhAmYEe_JJ_IN4g.png" data-width="220" data-height="489" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_d17quX4zhAmYEe_JJ_IN4g.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="33" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*d17quX4zhAmYEe_JJ_IN4g.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_d17quX4zhAmYEe_JJ_IN4g(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*d17quX4zhAmYEe_JJ_IN4g.png"></noscript></div></div></figure><figure name="a2de" id="a2de" class="graf graf--figure graf--iframe graf-after--figure"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/d9600bc0df919950bed4057878bb0409?postId=8613c6b6d2cd" data-media-id="d9600bc0df919950bed4057878bb0409" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/d9600bc0df919950bed4057878bb0409.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/d9600bc0df919950bed4057878bb0409?postId=8613c6b6d2cd" data-media-id="d9600bc0df919950bed4057878bb0409" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="480c" id="480c" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 499px; max-height: 744px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 149.10000000000002%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*a03Xr_5-wfKDzWoqHUiyyg.png" data-width="499" data-height="744" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_a03Xr_5-wfKDzWoqHUiyyg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="50" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*a03Xr_5-wfKDzWoqHUiyyg.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_a03Xr_5-wfKDzWoqHUiyyg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*a03Xr_5-wfKDzWoqHUiyyg.png"></noscript></div></div></figure><p name="ec7e" id="ec7e" class="graf graf--p graf-after--figure">We see that the tree used the following 5 values to evaluate by age: 43.5, 19, 22.5, 30, and 32 years. If you look closely, these are exactly the mean values between the ages at which the target class “switches” from 1 to 0 or 0 to 1. To illustrate further, 43.5 is the average of 38 and 49 years; a 38-year-old customer failed to return the loan whereas the 49-year-old did. The tree looks for the values at which the target class switches its value as a threshold for “cutting” a quantitative variable.</p><p name="9476" id="9476" class="graf graf--p graf-after--p">Given this information, why do you think it makes no sense here to consider a feature like “Age &lt;17.5”?</p><p name="8e13" id="8e13" class="graf graf--p graf-after--p">Let’s consider a more complex example by adding the “Salary” variable (in the thousands of dollars per year).</p><figure name="af52" id="af52" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.714%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/abc0df2a49847ff6bd541717eee0ba0d?postId=8613c6b6d2cd" data-media-id="abc0df2a49847ff6bd541717eee0ba0d" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/abc0df2a49847ff6bd541717eee0ba0d.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/abc0df2a49847ff6bd541717eee0ba0d?postId=8613c6b6d2cd" data-media-id="abc0df2a49847ff6bd541717eee0ba0d" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="d782" id="d782" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 292px; max-height: 494px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 169.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*6VJBLqoPTy07sJLQa3iNrQ.png" data-width="292" data-height="494" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_6VJBLqoPTy07sJLQa3iNrQ.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="44" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*6VJBLqoPTy07sJLQa3iNrQ.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_6VJBLqoPTy07sJLQa3iNrQ(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*6VJBLqoPTy07sJLQa3iNrQ.png"></noscript></div></div></figure><p name="10ce" id="10ce" class="graf graf--p graf-after--figure">If we sort by age, the target class ( “loan default”) switches (from 1 to 0 or vice versa) 5 times. And if we sort by salary, it switches 7 times. How will the tree choose features now? Let’s see.</p><figure name="1826" id="1826" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/ebf0b04c2d2276e599cedb424e119acd?postId=8613c6b6d2cd" data-media-id="ebf0b04c2d2276e599cedb424e119acd" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/ebf0b04c2d2276e599cedb424e119acd.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/ebf0b04c2d2276e599cedb424e119acd?postId=8613c6b6d2cd" data-media-id="ebf0b04c2d2276e599cedb424e119acd" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="3a74" id="3a74" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 505px; max-height: 611px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 121%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*pZbJ-J1Q_UW-AH1KrhbiBA.png" data-width="505" data-height="611" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_pZbJ-J1Q_UW-AH1KrhbiBA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="62" height="75"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*pZbJ-J1Q_UW-AH1KrhbiBA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_pZbJ-J1Q_UW-AH1KrhbiBA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*pZbJ-J1Q_UW-AH1KrhbiBA.png"></noscript></div></div></figure><p name="adb3" id="adb3" class="graf graf--p graf-after--figure">We see that the tree partitioned by both salary and age. Moreover, the thresholds for feature comparisons are 43.5 and 22.5 years of age and 95k and 30.5k per year. Again, we see that 95 is the average between 88 and 102; the individual with a salary of 88k proved to be “bad” while the one with 102k was “good”. The same goes for 30.5k. That is, only a few values for comparisons by age and salary were searched. Why did the tree choose these features? Because they gave better partitioning (according to Gini uncertainty).</p><p name="02d7" id="02d7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Conclusion</strong>: the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes.</p><p name="a3b2" id="a3b2" class="graf graf--p graf-after--p">Furthermore, when there are a lot of numeric features in a dataset, each with a lot of unique values, only the top-N of the thresholds described above are selected, i.e. only use the top-N that give maximum gain. The process is to construct a tree of depth 1, compute the entropy (or Gini uncertainty), and select the best thresholds for comparison.</p><p name="dc71" id="dc71" class="graf graf--p graf-after--p">To illustrate, if we split by “Salary ≤ 34.5”, the left subgroup will have the entropy of 0 (all clients are “bad”), and the right one will have the entropy of 0.954 (3 “bad” and 5 “good”, you can check this yourself as it will be part of the assignment). The information gain is roughly 0.3. If we split by “Salary ≤ 95”, the left subgroup will have an entropy of 0.97 (6 “bad” and 4 “good”), and the right one will have an entropy of 0 (a group containing only one object). The information gain is about 0.11. If we calculate information gain for each partition in that manner, we can select the thresholds for comparison of each numeric feature before the construction of a large tree (using all features).</p><p name="8250" id="8250" class="graf graf--p graf-after--p">More examples of numeric feature discretization can be found in posts like <a href="http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/" data-href="http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this</a> or <a href="http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx" data-href="http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this</a>. One of the most prominent scientific papers on this subject is “On the handling of continuous-valued attributes in decision tree generation” (UM Fayyad. KB Irani, “Machine Learning”, 1992).</p><h3 name="8b95" id="8b95" class="graf graf--h3 graf-after--p">Crucial Tree Parameters</h3><p name="dcee" id="dcee" class="graf graf--p graf-after--h3">Technically, you can build a decision tree until each leaf has exactly one instance, but this is not common in practice when building a single tree because it will be <em class="markup--em markup--p-em">overfitted</em>, or too tuned to the training set, and will not predict labels for new data well. At the bottom of the tree, at some great depth, there will be partitions on less important features (e.g. whether a client came from Leeds or New York). We can exaggerate this story further and find that all four clients who came to the bank for a loan in green trousers did not return the loan. Even if that were true in training, we do not want our classification model to generate such specific rules.</p><p name="6dcf" id="6dcf" class="graf graf--p graf-after--p">There are two exceptions where the trees are built to the maximum depth:</p><ul class="postList"><li name="6578" id="6578" class="graf graf--li graf-after--p">Random Forest (a group of trees) averages the responses from individual trees that are built to the maximum depth (we will talk later on why you should do this)</li><li name="e13c" id="e13c" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">Pruning</em> trees. In this approach, the tree is first constructed to the maximum depth. Then, from the bottom up, some nodes of the tree are removed by comparing the quality of the tree with and without that partition (comparison is performed using <em class="markup--em markup--li-em">cross-validation</em>, more on this below).</li></ul><p name="f68e" id="f68e" class="graf graf--p graf-after--li">The picture below is an example of a dividing border built in an overfitted tree.</p><figure name="5bc7" id="5bc7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 590px; max-height: 465px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 78.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*Lg34OEoe_VlX2-1n.png" data-width="590" data-height="465" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_Lg34OEoe_VlX2-1n.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="58"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/0*Lg34OEoe_VlX2-1n.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_Lg34OEoe_VlX2-1n(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*Lg34OEoe_VlX2-1n.png"></noscript></div></div></figure><p name="d9f9" id="d9f9" class="graf graf--p graf-after--figure">The most common ways to deal with overfitting in decision trees are as follows:</p><ul class="postList"><li name="6054" id="6054" class="graf graf--li graf-after--p">artificial limitation of the depth or a minimum number of samples in the leaves: the construction of a tree just stops at some point;</li><li name="8ce7" id="8ce7" class="graf graf--li graf-after--li">pruning the tree.</li></ul><h3 name="9b72" id="9b72" class="graf graf--h3 graf-after--li">Class DecisionTreeClassifier in Scikit-learn</h3><p name="d1c7" id="d1c7" class="graf graf--p graf-after--h3">The main parameters of the <code class="markup--code markup--p-code"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" data-href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">sklearn.tree.DecisionTreeClassifier</a></code> class are:</p><ul class="postList"><li name="81ce" id="81ce" class="graf graf--li graf-after--p"><code class="markup--code markup--li-code">max_depth</code> – the maximum depth of the tree;</li><li name="d31e" id="d31e" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">max_features</code> – the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be "expensive" to search for partitions for <em class="markup--em markup--li-em">all</em> features);</li><li name="7227" id="7227" class="graf graf--li graf-after--li"><code class="markup--code markup--li-code">min_samples_leaf</code> – the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.</li></ul><p name="8874" id="8874" class="graf graf--p graf-after--li">The parameters of the tree need to be set depending on input data, and it is usually done by means of <em class="markup--em markup--p-em">cross-validation</em>, more on this below.</p><h3 name="4b9f" id="4b9f" class="graf graf--h3 graf-after--p">Decision Tree in a Regression Problem</h3><p name="0f08" id="0f08" class="graf graf--p graf-after--h3">When predicting a numeric variable, the idea of a tree construction remains the same, but the quality criteria changes.</p><figure name="af04" id="af04" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 455px; max-height: 135px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 29.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*iiqWU5CR4F1F_8aQFaOpvg.png" data-width="455" data-height="135" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_iiqWU5CR4F1F_8aQFaOpvg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="21"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*iiqWU5CR4F1F_8aQFaOpvg.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_iiqWU5CR4F1F_8aQFaOpvg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*iiqWU5CR4F1F_8aQFaOpvg.png"></noscript></div></div><figcaption class="imageCaption">Variance around the&nbsp;mean</figcaption></figure><p name="4e3a" id="4e3a" class="graf graf--p graf-after--figure">where <em class="markup--em markup--p-em">n</em> is the number of samples in a leaf, <em class="markup--em markup--p-em">Yi </em>is the value of the target variable. Simply put, by minimizing the variance around the mean, we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.</p><h4 name="9017" id="9017" class="graf graf--h4 graf-after--p">Example</h4><p name="90a5" id="90a5" class="graf graf--p graf-after--h4">Let’s generate some data distributed by the function</p><figure name="dd75" id="dd75" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 257px; max-height: 43px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.7%;"></div><img class="graf-image" data-image-id="1*JlU7gNyjvAJDLc9hbLJVKg.png" data-width="257" data-height="43" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_JlU7gNyjvAJDLc9hbLJVKg.png"></div></figure><p name="3847" id="3847" class="graf graf--p graf-after--figure">with some noise. Then we will train a tree on it and show what predictions it makes.</p><figure name="4c42" id="4c42" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 87.857%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/fa9377033bbe22e1bcd9e2f6439d8b5a?postId=8613c6b6d2cd" data-media-id="fa9377033bbe22e1bcd9e2f6439d8b5a" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/fa9377033bbe22e1bcd9e2f6439d8b5a.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/fa9377033bbe22e1bcd9e2f6439d8b5a?postId=8613c6b6d2cd" data-media-id="fa9377033bbe22e1bcd9e2f6439d8b5a" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="7012" id="7012" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 604px; max-height: 370px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*-_dzWXtxS6XpmZemkKacnw.png" data-width="604" data-height="370" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_-_dzWXtxS6XpmZemkKacnw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="45"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*-_dzWXtxS6XpmZemkKacnw.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_-_dzWXtxS6XpmZemkKacnw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*-_dzWXtxS6XpmZemkKacnw.png"></noscript></div></div></figure><p name="e255" id="e255" class="graf graf--p graf-after--figure">We see that the decision tree approximates the data with a piecewise constant function.</p><h3 name="267c" id="267c" class="graf graf--h3 graf-after--p">3. Nearest Neighbors Method</h3><p name="2627" id="2627" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">The nearest neighbors method</em> (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.</p><p name="c305" id="c305" class="graf graf--p graf-after--p">According to the nearest neighbors method, the green ball would be classified as “blue” rather than “red”.</p><figure name="e830" id="e830" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 404px; max-height: 262px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*TMLaS4woI6xo6RKs.png" data-width="404" data-height="262" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_TMLaS4woI6xo6RKs.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="47"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/0*TMLaS4woI6xo6RKs.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_TMLaS4woI6xo6RKs(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*TMLaS4woI6xo6RKs.png"></noscript></div></div></figure><p name="8a93" id="8a93" class="graf graf--p graf-after--figure">For another example, if you do not know how to tag a Bluetooth-headset on an online listing, you can find 5 similar headsets, and, if 4 of them are tagged as “accessories” and only 1 as “Technology”, then you will also label it under “accessories”.</p><p name="1be0" id="1be0" class="graf graf--p graf-after--p">To classify each sample from the test set, one needs to perform the following operations in order:</p><ol class="postList"><li name="5b71" id="5b71" class="graf graf--li graf-after--p">Calculate the distance to each of the samples in the training set.</li><li name="109c" id="109c" class="graf graf--li graf-after--li">Select <em class="markup--em markup--li-em">k</em> samples from the training set with the minimal distance to them.</li><li name="28e4" id="28e4" class="graf graf--li graf-after--li">The class of the test sample will be the most frequent class among those <em class="markup--em markup--li-em">k</em> nearest neighbors.</li></ol><p name="491c" id="491c" class="graf graf--p graf-after--li">The method adapts quite easily for the regression problem: on step 3, it returns not the class, but the number — a mean (or median) of the target variable among neighbors.</p><p name="4721" id="4721" class="graf graf--p graf-after--p">A notable feature of this approach is its laziness — calculations are only done during the prediction phase, when a test sample needs to be classified. No model is constructed from the training examples beforehand. In contrast, recall that for decision trees in the first half of this article the tree is constructed based on the training set, and the classification of test cases occurs relatively quickly by traversing through the tree.</p><p name="3421" id="3421" class="graf graf--p graf-after--p">Nearest neighbors is a well-studied approach. There exist many important theorems claiming that, on “endless” datasets, it is the optimal method of classification. The authors of the classic book “The Elements of Statistical Learning” consider k-NN to be a theoretically ideal algorithm which usage is only limited by computation power and the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" data-href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">curse of dimensionality</a>.</p><h3 name="c787" id="c787" class="graf graf--h3 graf-after--p">Nearest Neighbors Method in Real Applications</h3><ul class="postList"><li name="2456" id="2456" class="graf graf--li graf-after--h3">k-NN can serve as a good starting point (baseline) in some cases;</li><li name="a82c" id="a82c" class="graf graf--li graf-after--li">In Kaggle competitions, k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending;</li><li name="c7cf" id="c7cf" class="graf graf--li graf-after--li">The nearest neighbors method extends to other tasks like recommendation systems. The initial decision could be a recommendation of a product (or service) that is popular among the <em class="markup--em markup--li-em">closest neighbors</em> of the person for whom we want to make a recommendation;</li><li name="6bac" id="6bac" class="graf graf--li graf-after--li">In practice, on large datasets, approximate methods of search are often used for nearest neighbors. There is a number of open source libraries that implement such algorithms; check out Spotify’s library <a href="https://github.com/spotify/annoy" data-href="https://github.com/spotify/annoy" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">Annoy</a>.</li></ul><p name="3095" id="3095" class="graf graf--p graf-after--li">The quality of classification/regression with k-NN depends on several parameters:</p><ul class="postList"><li name="b520" id="b520" class="graf graf--li graf-after--p">The number of neighbors <em class="markup--em markup--li-em">k</em>.</li><li name="d3b6" id="d3b6" class="graf graf--li graf-after--li">The distance measure between samples (common ones include Hamming, Euclidean, cosine, and Minkowski distances). Note that most of these metrics require data to be scaled. Simply speaking, we do not want the “salary” feature, which is on the order of thousands, to affect the distance more than “age”, which is generally less than 100.</li><li name="301d" id="301d" class="graf graf--li graf-after--li">Weights of neighbors (each neighbor may contribute different weights; for example, the further the sample, the lower the weight).</li></ul><h3 name="12e4" id="12e4" class="graf graf--h3 graf-after--li">Class <code class="markup--code markup--h3-code">KNeighborsClassifier</code> in Scikit-learn</h3><p name="960e" id="960e" class="graf graf--p graf-after--h3">The main parameters of the class <code class="markup--code markup--p-code">sklearn.neighbors.KNeighborsClassifier</code> are:</p><ul class="postList"><li name="122b" id="122b" class="graf graf--li graf-after--p">weights: <code class="markup--code markup--li-code">uniform</code> (all weights are equal), <code class="markup--code markup--li-code">distance</code> (the weight is inversely proportional to the distance from the test sample), or any other user-defined function;</li><li name="4d06" id="4d06" class="graf graf--li graf-after--li">algorithm (optional): <code class="markup--code markup--li-code">brute</code>, <code class="markup--code markup--li-code">ball_tree</code>, <code class="markup--code markup--li-code">KD_tree</code>, or <code class="markup--code markup--li-code">auto</code>. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to <code class="markup--code markup--li-code">auto</code>, the right way to find the neighbors will be automatically chosen based on the training set.</li><li name="bd2e" id="bd2e" class="graf graf--li graf-after--li">leaf_size (optional): threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;</li><li name="8886" id="8886" class="graf graf--li graf-after--li">metric: <code class="markup--code markup--li-code">minkowski</code>, <code class="markup--code markup--li-code">manhattan</code>, <code class="markup--code markup--li-code">euclidean</code>, <code class="markup--code markup--li-code">chebyshev</code>, or other.</li></ul><h3 name="1405" id="1405" class="graf graf--h3 graf-after--li">4. Choosing Model Parameters and Cross-Validation</h3><p name="de4e" id="de4e" class="graf graf--p graf-after--h3">The main task of learning algorithms is to be able to <em class="markup--em markup--p-em">generalize</em> to unseen data. Since we cannot immediately check the model performance on new, incoming data (because we do not know the true values of the target variable yet), it is necessary to sacrifice a small portion of the data to check the quality of the model on it.</p><p name="b81a" id="b81a" class="graf graf--p graf-after--p">This is often done in one of two ways:</p><ul class="postList"><li name="fce6" id="fce6" class="graf graf--li graf-after--p">setting aside a part of the dataset (<em class="markup--em markup--li-em">held-out/hold-out set</em>). We reserve a fraction of the training set (typically from 20% to 40%), train the model on the remaining data (60–80% of the original set), and compute performance metrics for the model (e.g accuracy) on the hold-out set.</li><li name="af5d" id="af5d" class="graf graf--li graf-after--li"><em class="markup--em markup--li-em">cross-validation</em>. The most frequent case here is <em class="markup--em markup--li-em">k-fold cross-validation</em>.</li></ul></div><div class="section-inner sectionLayout--outsetColumn"><figure name="a1cf" id="a1cf" class="graf graf--figure graf--layoutOutsetCenter graf-after--li" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 302px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="0*iGVu2OmIL1WUSjZF.png" data-width="1000" data-height="302" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_iGVu2OmIL1WUSjZF.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="22"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/0*iGVu2OmIL1WUSjZF.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_iGVu2OmIL1WUSjZF(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/0*iGVu2OmIL1WUSjZF.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="cd64" id="cd64" class="graf graf--p graf-after--figure">In k-fold cross-validation, the model is trained <em class="markup--em markup--p-em">K</em> times on different (<em class="markup--em markup--p-em">K-1</em>) subsets of the original dataset (in white) and checked on the remaining subset (each time a different one, shown above in orange). We obtain <em class="markup--em markup--p-em">K</em> model quality assessments that are usually averaged to give an overall average quality of classification/regression.</p><p name="3590" id="3590" class="graf graf--p graf-after--p">Cross-validation provides a better assessment of the model quality on new data compared to the hold-out set approach. However, cross-validation is computationally expensive when you have a lot of data.</p><p name="04ad" id="04ad" class="graf graf--p graf-after--p">Cross-validation is a very important technique in machine learning and can also be applied in statistics and econometrics. It helps with hyperparameter tuning, model comparison, feature evaluation, etc. More details can be found <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html" data-href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here</a> (blog post by Sebastian Raschka) or in any classic textbook on machine (statistical) learning.</p><h3 name="1229" id="1229" class="graf graf--h3 graf-after--p">5. Application Examples and Complex&nbsp;Cases</h3><h3 name="d662" id="d662" class="graf graf--h3 graf-after--h3">Decision trees and nearest neighbors method in a customer churn prediction task</h3><p name="be35" id="be35" class="graf graf--p graf-after--h3">Let’s read data into a <code class="markup--code markup--p-code">DataFrame</code> and preprocess it. Store <em class="markup--em markup--p-em">State</em> in a separate <code class="markup--code markup--p-code">Series</code> object for now and remove it from the dataframe. We will train the first model without the <em class="markup--em markup--p-em">State</em> feature, and then we will see if it helps.</p><figure name="6034" id="6034" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/d2df271a0611166325e3f2d13e7da48e?postId=8613c6b6d2cd" data-media-id="d2df271a0611166325e3f2d13e7da48e" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/d2df271a0611166325e3f2d13e7da48e.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/d2df271a0611166325e3f2d13e7da48e?postId=8613c6b6d2cd" data-media-id="d2df271a0611166325e3f2d13e7da48e" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="d848" id="d848" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 133px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*_ayZZheYD8jFveMgXZ_7fA.png" data-width="988" data-height="188" data-action="zoom" data-action-value="1*_ayZZheYD8jFveMgXZ_7fA.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1__ayZZheYD8jFveMgXZ_7fA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="13"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*_ayZZheYD8jFveMgXZ_7fA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1__ayZZheYD8jFveMgXZ_7fA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*_ayZZheYD8jFveMgXZ_7fA.png"></noscript></div></div></figure><p name="4ed7" id="4ed7" class="graf graf--p graf-after--figure">Let’s allocate 70% of the set for training (<code class="markup--code markup--p-code">X_train</code>, <code class="markup--code markup--p-code">y_train</code>) and 30% for the hold-out set (<code class="markup--code markup--p-code">X_holdout</code>, <code class="markup--code markup--p-code">y_holdout</code>). The hold-out set will not be involved in tuning the parameters of the models. We'll use it at the end, after tuning, to assess the quality of the resulting model. Let's train 2 models: a decision tree and k-NN. We do not know what parameters are good, so we will assume some random ones: a tree depth of 5 and the number of nearest neighbors equal 10.</p><figure name="ef2a" id="ef2a" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 31.286%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/c50323ecd9e9d8ead17911341cb3b122?postId=8613c6b6d2cd" data-media-id="c50323ecd9e9d8ead17911341cb3b122" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/c50323ecd9e9d8ead17911341cb3b122.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/c50323ecd9e9d8ead17911341cb3b122?postId=8613c6b6d2cd" data-media-id="c50323ecd9e9d8ead17911341cb3b122" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="6cec" id="6cec" class="graf graf--p graf-after--figure">Let’s assess prediction quality on our hold-out set with a simple metric — the proportion of correct answers (accuracy). The decision tree did better — percentage of correct answers is about 94% (decision tree) versus 88% (k-NN). Note that this performance is achieved by using random parameters.</p><figure name="fe75" id="fe75" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.857%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/12e78c34ff52832de27915dfe278b686?postId=8613c6b6d2cd" data-media-id="12e78c34ff52832de27915dfe278b686" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/12e78c34ff52832de27915dfe278b686.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/12e78c34ff52832de27915dfe278b686?postId=8613c6b6d2cd" data-media-id="12e78c34ff52832de27915dfe278b686" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="9a5a" id="9a5a" class="graf graf--p graf-after--figure">Now, let’s identify the parameters for the tree using cross-validation. We’ll tune the maximum depth and the maximum number of features used at each split. Here is the essence of how the GridSearchCV works: for each unique pair of values of <code class="markup--code markup--p-code">max_depth</code> and <code class="markup--code markup--p-code">max_features</code>, compute model performance with 5-fold cross-validation, and then select the best combination of parameters.</p><figure name="6008" id="6008" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.571%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/f203d8b929e433a538f1e44a0447e2ae?postId=8613c6b6d2cd" data-media-id="f203d8b929e433a538f1e44a0447e2ae" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/f203d8b929e433a538f1e44a0447e2ae.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/f203d8b929e433a538f1e44a0447e2ae?postId=8613c6b6d2cd" data-media-id="f203d8b929e433a538f1e44a0447e2ae" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="2d54" id="2d54" class="graf graf--p graf-after--figure">Let’s list the best parameters and the corresponding mean accuracy from cross-validation.</p><figure name="260b" id="260b" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 15.571%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/4d88a39f1ed85538a4b253c7fba32e6f?postId=8613c6b6d2cd" data-media-id="4d88a39f1ed85538a4b253c7fba32e6f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/4d88a39f1ed85538a4b253c7fba32e6f.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/4d88a39f1ed85538a4b253c7fba32e6f?postId=8613c6b6d2cd" data-media-id="4d88a39f1ed85538a4b253c7fba32e6f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="f956" id="f956" class="graf graf--p graf-after--figure">Let’s draw the resulting tree. Due to the fact that it is not entirely a toy example (its maximum depth is 6), the picture is not that small, but you can “walk” over the tree if you locally open the corresponding picture downloaded from the course repo.</p><figure name="223a" id="223a" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.857%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/7b9b552e1176b50bc0bc267d9a5cdd79?postId=8613c6b6d2cd" data-media-id="7b9b552e1176b50bc0bc267d9a5cdd79" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/7b9b552e1176b50bc0bc267d9a5cdd79.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/7b9b552e1176b50bc0bc267d9a5cdd79?postId=8613c6b6d2cd" data-media-id="7b9b552e1176b50bc0bc267d9a5cdd79" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure></div><div class="section-inner sectionLayout--fullWidth"><figure name="440b" id="440b" class="graf graf--figure graf--layoutFillWidth graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*yBZp1DXgtxBHsi8UxeC6Cg.png" data-width="4431" data-height="877" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_yBZp1DXgtxBHsi8UxeC6Cg.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="13"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*yBZp1DXgtxBHsi8UxeC6Cg.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_yBZp1DXgtxBHsi8UxeC6Cg(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*yBZp1DXgtxBHsi8UxeC6Cg.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="e488" id="e488" class="graf graf--p graf-after--figure">Now, let’s tune the number of neighbors <em class="markup--em markup--p-em">k</em> for k-NN:</p><figure name="0c2e" id="0c2e" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.571%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/b44bdf193962dfd5166d8014e95e895d?postId=8613c6b6d2cd" data-media-id="b44bdf193962dfd5166d8014e95e895d" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/b44bdf193962dfd5166d8014e95e895d.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/b44bdf193962dfd5166d8014e95e895d?postId=8613c6b6d2cd" data-media-id="b44bdf193962dfd5166d8014e95e895d" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="46e9" id="46e9" class="graf graf--p graf-after--figure">Here, the tree proved to be better than the nearest neighbors algorithm: 94.2%/96.6% accuracy for cross-validation and hold-out respectively. Decision trees perform very well, and even random forest (let’s think of it for now as a bunch of trees that work better together) in this example cannot achieve better performance (95.1%/95.3%) despite being trained for much longer.</p><figure name="ae24" id="ae24" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 53.286%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/8623f3bc35337b021f651c2b6f380456?postId=8613c6b6d2cd" data-media-id="8623f3bc35337b021f651c2b6f380456" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/8623f3bc35337b021f651c2b6f380456.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/8623f3bc35337b021f651c2b6f380456?postId=8613c6b6d2cd" data-media-id="8623f3bc35337b021f651c2b6f380456" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><h3 name="c89e" id="c89e" class="graf graf--h3 graf-after--figure">Complex Case for Decision&nbsp;Trees</h3><p name="8192" id="8192" class="graf graf--p graf-after--h3">To continue the discussion of the pros and cons of the methods in question, let’s consider a simple classification task, where a tree would perform well but does it in an “overly complicated” manner. Let’s create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line.</p><figure name="6e78" id="6e78" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/eb61c66d2367a4a7732b668abfce8ca7?postId=8613c6b6d2cd" data-media-id="eb61c66d2367a4a7732b668abfce8ca7" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/eb61c66d2367a4a7732b668abfce8ca7.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/eb61c66d2367a4a7732b668abfce8ca7?postId=8613c6b6d2cd" data-media-id="eb61c66d2367a4a7732b668abfce8ca7" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="8c38" id="8c38" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 590px; max-height: 465px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 78.8%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*oKfEmX5DBMev8e_Hqnv53g.png" data-width="590" data-height="465" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_oKfEmX5DBMev8e_Hqnv53g.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="58"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*oKfEmX5DBMev8e_Hqnv53g.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_oKfEmX5DBMev8e_Hqnv53g(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*oKfEmX5DBMev8e_Hqnv53g.png"></noscript></div></div></figure><p name="2af6" id="2af6" class="graf graf--p graf-after--figure">However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the 30 x 30 squares that frame the training set.</p><figure name="ede6" id="ede6" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/7ae048fab32bcc36d0de556428ba04e1?postId=8613c6b6d2cd" data-media-id="7ae048fab32bcc36d0de556428ba04e1" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/7ae048fab32bcc36d0de556428ba04e1.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/7ae048fab32bcc36d0de556428ba04e1?postId=8613c6b6d2cd" data-media-id="7ae048fab32bcc36d0de556428ba04e1" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="d42e" id="d42e" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 560px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*_OqTKeKHCUx0J38T7gGTYw.png" data-width="3000" data-height="2400" data-action="zoom" data-action-value="1*_OqTKeKHCUx0J38T7gGTYw.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1__OqTKeKHCUx0J38T7gGTYw.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="60"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*_OqTKeKHCUx0J38T7gGTYw.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1__OqTKeKHCUx0J38T7gGTYw(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*_OqTKeKHCUx0J38T7gGTYw.png"></noscript></div></div></figure><p name="203b" id="203b" class="graf graf--p graf-after--figure">We got this overly complex construction, although the solution is just a straight line <em class="markup--em markup--p-em">x1 = x2</em>.</p><figure name="cf60" id="cf60" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.857%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/1af7e72326579eeeec067b07233bc5d6?postId=8613c6b6d2cd" data-media-id="1af7e72326579eeeec067b07233bc5d6" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1af7e72326579eeeec067b07233bc5d6.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/1af7e72326579eeeec067b07233bc5d6?postId=8613c6b6d2cd" data-media-id="1af7e72326579eeeec067b07233bc5d6" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure></div><div class="section-inner sectionLayout--outsetColumn"><figure name="da44" id="da44" class="graf graf--figure graf--layoutOutsetCenter graf-after--figure" data-scroll="native"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 288px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.799999999999997%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*oDkLhpY3kfa87UsjIp51ZA.png" data-width="2583" data-height="744" data-action="zoom" data-action-value="1*oDkLhpY3kfa87UsjIp51ZA.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_oDkLhpY3kfa87UsjIp51ZA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="21"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*oDkLhpY3kfa87UsjIp51ZA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_oDkLhpY3kfa87UsjIp51ZA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*oDkLhpY3kfa87UsjIp51ZA.png"></noscript></div></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="acc3" id="acc3" class="graf graf--p graf-after--figure">The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our <a href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220" data-href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220" class="markup--anchor markup--p-anchor" target="_blank">next topic</a>).</p><figure name="5cc0" id="5cc0" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/6b4d3434376c2d263163367449a149e3?postId=8613c6b6d2cd" data-media-id="6b4d3434376c2d263163367449a149e3" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/6b4d3434376c2d263163367449a149e3.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/6b4d3434376c2d263163367449a149e3?postId=8613c6b6d2cd" data-media-id="6b4d3434376c2d263163367449a149e3" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="9729" id="9729" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 596px; max-height: 479px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.4%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*CSUaToczckWZGPf7wsKAKA.png" data-width="596" data-height="479" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_CSUaToczckWZGPf7wsKAKA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="60"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*CSUaToczckWZGPf7wsKAKA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_CSUaToczckWZGPf7wsKAKA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*CSUaToczckWZGPf7wsKAKA.png"></noscript></div></div></figure><h3 name="d97c" id="d97c" class="graf graf--h3 graf-after--figure">Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition</h3><p name="ae33" id="ae33" class="graf graf--p graf-after--h3">Now let’s have a look at how these 2 algorithms perform on a real-world task. We will use the <code class="markup--code markup--p-code">sklearn</code> built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.</p><p name="bb4e" id="bb4e" class="graf graf--p graf-after--p">Pictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is ​​”unfolded” into a vector of length 64, and we obtain a feature description of an object.</p><p name="e552" id="e552" class="graf graf--p graf-after--p">Let’s draw some handwritten digits. We see that they are distinguishable.</p><figure name="b1ec" id="b1ec" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 25%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/eeb9bf710715c6ef43864db717033080?postId=8613c6b6d2cd" data-media-id="eeb9bf710715c6ef43864db717033080" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/eeb9bf710715c6ef43864db717033080.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/eeb9bf710715c6ef43864db717033080?postId=8613c6b6d2cd" data-media-id="eeb9bf710715c6ef43864db717033080" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="37f0" id="37f0" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 271px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 38.7%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*u8zadz3PKNo3fOWFJm_vDA.png" data-width="919" data-height="356" data-action="zoom" data-action-value="1*u8zadz3PKNo3fOWFJm_vDA.png" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_u8zadz3PKNo3fOWFJm_vDA.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="28"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*u8zadz3PKNo3fOWFJm_vDA.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_u8zadz3PKNo3fOWFJm_vDA(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*u8zadz3PKNo3fOWFJm_vDA.png"></noscript></div></div></figure><p name="f8e0" id="f8e0" class="graf graf--p graf-after--figure">Next, let’s do the same experiment as in the previous task, but, this time, let’s change the ranges for tunable parameters.</p><p name="e599" id="e599" class="graf graf--p graf-after--p">Let’s select 70% of the dataset for training (<code class="markup--code markup--p-code">X_train</code>, <code class="markup--code markup--p-code">y_train</code>) and 30% for holdout (<code class="markup--code markup--p-code">X_holdout</code>, <code class="markup--code markup--p-code">y_holdout</code>). The holdout set will not participate in model parameters tuning; we will use it at the end to check the quality of the resulting model.</p><figure name="bd9a" id="bd9a" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.429%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/d18f2fa52ae89eb4182722b57dde7a19?postId=8613c6b6d2cd" data-media-id="d18f2fa52ae89eb4182722b57dde7a19" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/d18f2fa52ae89eb4182722b57dde7a19.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/d18f2fa52ae89eb4182722b57dde7a19?postId=8613c6b6d2cd" data-media-id="d18f2fa52ae89eb4182722b57dde7a19" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="a477" id="a477" class="graf graf--p graf-after--figure">Let’s train a decision tree and k-NN with our random parameters and make predictions on our holdout set. We can see that k-NN did much better, but note that this is with random parameters.</p><figure name="571c" id="571c" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 18.714%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/af1edbb0c7f4af8a297f4b289f55d815?postId=8613c6b6d2cd" data-media-id="af1edbb0c7f4af8a297f4b289f55d815" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/af1edbb0c7f4af8a297f4b289f55d815.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/af1edbb0c7f4af8a297f4b289f55d815?postId=8613c6b6d2cd" data-media-id="af1edbb0c7f4af8a297f4b289f55d815" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="8b35" id="8b35" class="graf graf--figure graf--iframe graf-after--figure"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.857%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/2aed2efa0ee6555018d2ccecd4f3c680?postId=8613c6b6d2cd" data-media-id="2aed2efa0ee6555018d2ccecd4f3c680" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/2aed2efa0ee6555018d2ccecd4f3c680.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/2aed2efa0ee6555018d2ccecd4f3c680?postId=8613c6b6d2cd" data-media-id="2aed2efa0ee6555018d2ccecd4f3c680" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="c2fb" id="c2fb" class="graf graf--p graf-after--figure">Now let’s tune our model parameters using cross-validation as before, but now we’ll take into account that we have more features than in the previous task: 64.</p><figure name="39b7" id="39b7" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 21.857%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/85b385de90910523b1bb8c1969db5bed?postId=8613c6b6d2cd" data-media-id="85b385de90910523b1bb8c1969db5bed" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/85b385de90910523b1bb8c1969db5bed.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/85b385de90910523b1bb8c1969db5bed?postId=8613c6b6d2cd" data-media-id="85b385de90910523b1bb8c1969db5bed" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="e12e" id="e12e" class="graf graf--p graf-after--figure">Let’s see the best parameters combination and the corresponding accuracy from cross-validation:</p><figure name="587b" id="587b" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.429%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/36b64aa1c99ee8d214c83773a7964bff?postId=8613c6b6d2cd" data-media-id="36b64aa1c99ee8d214c83773a7964bff" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/36b64aa1c99ee8d214c83773a7964bff.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/36b64aa1c99ee8d214c83773a7964bff?postId=8613c6b6d2cd" data-media-id="36b64aa1c99ee8d214c83773a7964bff" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="86a2" id="86a2" class="graf graf--p graf-after--figure">That has already passed 66% but not quite 97%. k-NN works better on this dataset. In the case of one nearest neighbor, we were able to reach 99% guesses on cross-validation.</p><figure name="9d9a" id="9d9a" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.429%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/9bb1f93487ab77f352d87d17165392ee?postId=8613c6b6d2cd" data-media-id="9bb1f93487ab77f352d87d17165392ee" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/9bb1f93487ab77f352d87d17165392ee.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/9bb1f93487ab77f352d87d17165392ee?postId=8613c6b6d2cd" data-media-id="9bb1f93487ab77f352d87d17165392ee" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="0315" id="0315" class="graf graf--p graf-after--figure">Let’s train a random forest on the same dataset, it works better than k-NN on the majority of datasets. But we here have an exception.</p><figure name="64b7" id="64b7" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 12.429%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/66ac4a6910d9a2ec12ce8721a974fb91?postId=8613c6b6d2cd" data-media-id="66ac4a6910d9a2ec12ce8721a974fb91" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/66ac4a6910d9a2ec12ce8721a974fb91.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/66ac4a6910d9a2ec12ce8721a974fb91?postId=8613c6b6d2cd" data-media-id="66ac4a6910d9a2ec12ce8721a974fb91" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="4bec" id="4bec" class="graf graf--p graf-after--figure">You would be right to point out that we have not tuned any <code class="markup--code markup--p-code">RandomForestClassifier</code> parameters here. Even with tuning, the training accuracy doesn’t reach 98% as it did with one nearest neighbor.</p><figure name="26d4" id="26d4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 288px; max-height: 205px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*9ZRg1FTetV7cMI_oNNcD_w.png" data-width="288" data-height="205" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_9ZRg1FTetV7cMI_oNNcD_w.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="52"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*9ZRg1FTetV7cMI_oNNcD_w.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_9ZRg1FTetV7cMI_oNNcD_w(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*9ZRg1FTetV7cMI_oNNcD_w.png"></noscript></div></div><figcaption class="imageCaption"><em class="markup--em markup--figure-em">CV and Holdout are average shares of the correct answers on cross-model validation and hold-out sample. DT stands for a decision tree, k-NN stands for k-nearest neighbors, RF stands for random&nbsp;forest</em></figcaption></figure><p name="4fb7" id="4fb7" class="graf graf--p graf-after--figure">The <strong class="markup--strong markup--p-strong">conclusion</strong> of this experiment (and general advice): first check simple models on your data: decision tree and nearest neighbors (next time we will also add logistic regression to this list). It might be the case that these methods already work well enough.</p><h3 name="4634" id="4634" class="graf graf--h3 graf-after--p">Complex Case for the Nearest Neighbors Method</h3><p name="7c03" id="7c03" class="graf graf--p graf-after--h3">Let’s consider another simple example. In the classification problem, one of the features will just be proportional to the vector of responses, but this won’t help for the nearest neighbors method.</p><figure name="e406" id="e406" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/559da4052b71b58187c316d9173cfde1?postId=8613c6b6d2cd" data-media-id="559da4052b71b58187c316d9173cfde1" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/559da4052b71b58187c316d9173cfde1.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/559da4052b71b58187c316d9173cfde1?postId=8613c6b6d2cd" data-media-id="559da4052b71b58187c316d9173cfde1" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="e9ff" id="e9ff" class="graf graf--p graf-after--figure">As always, we will look at the accuracy for cross-validation and the hold-out set. Let’s construct curves reflecting the dependence of these quantities on the <code class="markup--code markup--p-code">n_neighbors</code> parameter in the method of nearest neighbors. These curves are called validation curves.</p><p name="88dc" id="88dc" class="graf graf--p graf-after--p">One can see that k-NN with the Euclidean distance does not work well on the problem, even when you vary the number of nearest neighbors over a wide range.</p><figure name="83ee" id="83ee" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/52ee260c895c98755c6cb3ef99d3a6ce?postId=8613c6b6d2cd" data-media-id="52ee260c895c98755c6cb3ef99d3a6ce" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/52ee260c895c98755c6cb3ef99d3a6ce.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/52ee260c895c98755c6cb3ef99d3a6ce?postId=8613c6b6d2cd" data-media-id="52ee260c895c98755c6cb3ef99d3a6ce" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><figure name="0b9c" id="0b9c" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 598px; max-height: 479px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.10000000000001%;"></div><div class="progressiveMedia js-progressiveMedia graf-image is-canvasLoaded is-imageLoaded" data-image-id="1*_jt-Vi1_0IRsC0riZ1nb6g.png" data-width="598" data-height="479" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1__jt-Vi1_0IRsC0riZ1nb6g.png" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="60"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*_jt-Vi1_0IRsC0riZ1nb6g.png" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1__jt-Vi1_0IRsC0riZ1nb6g(1).png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*_jt-Vi1_0IRsC0riZ1nb6g.png"></noscript></div></div></figure><p name="048e" id="048e" class="graf graf--p graf-after--figure">In contrast, the decision tree easily “detects” hidden dependencies in the data despite a restriction on the maximum depth.</p><figure name="1b54" id="1b54" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.143%;"></div><div class="progressiveMedia js-progressiveMedia is-canvasLoaded is-imageLoaded" data-scroll="native"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/resize" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas" width="75" height="75"></canvas><div class="iframeContainer"><iframe width="700" height="250" data-src="/media/6b09e4c1be2d25a3ad3ebcdabdbb502f?postId=8613c6b6d2cd" data-media-id="6b09e4c1be2d25a3ad3ebcdabdbb502f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen="" frameborder="0" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/6b09e4c1be2d25a3ad3ebcdabdbb502f.html"></iframe></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/6b09e4c1be2d25a3ad3ebcdabdbb502f?postId=8613c6b6d2cd" data-media-id="6b09e4c1be2d25a3ad3ebcdabdbb502f" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><p name="e3be" id="e3be" class="graf graf--p graf-after--figure">In the second example, the tree solved the problem perfectly while k-NN experienced difficulties. However, this is more of a disadvantage of using Euclidian distance than of the method. It did not allow us to reveal that one feature was much better than the others.</p><h3 name="955e" id="955e" class="graf graf--h3 graf-after--p">6. Pros and Cons of Decision Trees and the Nearest Neighbors Method</h3><h4 name="95f1" id="95f1" class="graf graf--h4 graf-after--h3">Pros and cons of decision&nbsp;trees</h4><p name="c057" id="c057" class="graf graf--p graf-after--h4">Pros:</p><ul class="postList"><li name="3985" id="3985" class="graf graf--li graf-after--p">Generation of clear human-understandable classification rules, e.g. “if age &lt;25 and is interested in motorcycles, deny the loan”. This property is called interpretability of the model.</li><li name="e998" id="e998" class="graf graf--li graf-after--li">Decision trees can be easily visualized, i.e. both the model itself (the tree) and prediction for a certain test object (a path in the tree) can “be interpreted”.</li><li name="4d6e" id="4d6e" class="graf graf--li graf-after--li">Fast training and forecasting.</li><li name="7563" id="7563" class="graf graf--li graf-after--li">Small number of model parameters.</li><li name="3b73" id="3b73" class="graf graf--li graf-after--li">Supports both numerical and categorical features.</li></ul><p name="6aad" id="6aad" class="graf graf--p graf-after--li">Cons:</p><ul class="postList"><li name="d9cf" id="d9cf" class="graf graf--li graf-after--p">The trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). This impairs the interpretability of the model.</li><li name="2619" id="2619" class="graf graf--li graf-after--li">Separating border built by a decision tree has its limitations — it consists of hyperplanes perpendicular to one of the coordinate axes, which is inferior in quality to some other methods, in practice.</li><li name="5f2d" id="5f2d" class="graf graf--li graf-after--li">We need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. Note that overfitting is an issue for all machine learning methods.</li><li name="8e4f" id="8e4f" class="graf graf--li graf-after--li">Instability. Small changes to the data can significantly change the decision tree. This problem is tackled with decision tree ensembles (discussed next time).</li><li name="db79" id="db79" class="graf graf--li graf-after--li">The optimal decision tree search problem is NP-complete. Some heuristics are used in practice such as greedy search for a feature with maximum information gain, but it does not guarantee finding the globally optimal tree.</li><li name="7366" id="7366" class="graf graf--li graf-after--li">Difficulties to support missing values in the data. Friedman estimated that it took about 50% of the code to support gaps in data in CART (an improved version of this algorithm is implemented in <code class="markup--code markup--li-code">sklearn</code>).</li><li name="aa78" id="aa78" class="graf graf--li graf-after--li">The model can only interpolate but not extrapolate (the same is true for random forests and tree boosting). That is, a decision tree makes constant prediction for the objects that lie beyond the bounding box set by the training set in the feature space. In our example with the yellow and blue balls, it would mean that the model gives the same predictions for all balls with positions &gt;19 or &lt;0.</li></ul><h4 name="8193" id="8193" class="graf graf--h4 graf-after--li">Pros and cons of the nearest neighbors method</h4><p name="2509" id="2509" class="graf graf--p graf-after--h4">Pros:</p><ul class="postList"><li name="136e" id="136e" class="graf graf--li graf-after--p">Simple implementation.</li><li name="1c8b" id="1c8b" class="graf graf--li graf-after--li">Well studied.</li><li name="e7e2" id="e7e2" class="graf graf--li graf-after--li">Typically, the method is a good first solution not only for classification or regression, but also recommendations.</li><li name="e38e" id="e38e" class="graf graf--li graf-after--li">It can be adapted to a certain problem by choosing the right metrics or kernel (in a nutshell, the kernel may set the similarity operation for complex objects such as graphs while keeping the k-NN approach the same). By the way, <a href="https://www.kaggle.com/dyakonov" data-href="https://www.kaggle.com/dyakonov" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">Alexander Dyakonov</a>, a former top-1 kaggler, loves the simplest k-NN but with the tuned object similarity metric.</li><li name="69f7" id="69f7" class="graf graf--li graf-after--li">Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates (“We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset”).</li></ul><p name="8bd5" id="8bd5" class="graf graf--p graf-after--li">Cons:</p><ul class="postList"><li name="fb3f" id="fb3f" class="graf graf--li graf-after--p">Method considered fast in comparison with compositions of algorithms, but the number of neighbors used for classification is usually large (100–150) in real life, in which case the algorithm will not operate as fast as a decision tree.</li><li name="c43d" id="c43d" class="graf graf--li graf-after--li">If a dataset has many variables, it is difficult to find the right weights and to determine which features are not important for classification/regression.</li><li name="d99c" id="d99c" class="graf graf--li graf-after--li">Dependency on the selected distance metric between the objects. Selecting the Euclidean distance by default is often unfounded. You can find a good solution by grid searching over parameters, but this becomes very time consuming for large datasets.</li><li name="2e36" id="2e36" class="graf graf--li graf-after--li">There are no theoretical ways to choose the number of neighbors — only grid search (though this is often true for all hyperparameters of all models). In the case of a small number of neighbors, the method is sensitive to outliers, that is, it is inclined to overfit.</li><li name="7b5e" id="7b5e" class="graf graf--li graf-after--li">As a rule, it does not work well when there are a lot of features due to the “curse of dimensionality”. Professor Pedro Domingos, a well-known member in the ML community, talks about it <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" data-href="https://homes.cs.washington.edu/%7Epedrod/papers/cacm12.pdf" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">here</a> in his popular paper, “A Few Useful Things to Know about Machine Learning”; also “the curse of dimensionality” is described in the Deep Learning book in <a href="http://www.deeplearningbook.org/contents/ml.html" data-href="http://www.deeplearningbook.org/contents/ml.html" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">this chapter</a>.</li></ul><p name="9ece" id="9ece" class="graf graf--p graf-after--li">This is a lot of information, but, hopefully, this article will be a great reference for you for a long time&nbsp;:)</p><h3 name="e089" id="e089" class="graf graf--h3 graf-after--p">7. Assignment #3</h3><p name="9ea4" id="9ea4" class="graf graf--p graf-after--h3">A demo version is to appear soon (~ in the end of June 2018). Full version will be available in the new session of the course (starting on October, 1).</p><h3 name="93a3" id="93a3" class="graf graf--h3 graf-after--p">8. Useful resources</h3><ul class="postList"><li name="f545" id="f545" class="graf graf--li graf-after--h3">Decision trees and k Nearest Neighbors are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</li><li name="ccb1" id="ccb1" class="graf graf--li graf-after--li">The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</li><li name="4ad3" id="4ad3" class="graf graf--li graf-after--li"><a href="http://scikit-learn.org/stable/documentation.html" data-href="http://scikit-learn.org/stable/documentation.html" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">Scikit-learn</a> library. These guys work hard on writing really clear documentation.</li><li name="f98d" id="f98d" class="graf graf--li graf-after--li">Scipy 2017 <a href="https://github.com/amueller/scipy-2017-sklearn" data-href="https://github.com/amueller/scipy-2017-sklearn" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</li><li name="b75c" id="b75c" class="graf graf--li graf-after--li">One more <a href="https://github.com/diefimov/MTH594_MachineLearning" data-href="https://github.com/diefimov/MTH594_MachineLearning" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">ML course</a> with very good materials.</li><li name="f7a6" id="f7a6" class="graf graf--li graf-after--li"><a href="https://github.com/rushter/MLAlgorithms" data-href="https://github.com/rushter/MLAlgorithms" class="markup--anchor markup--li-anchor" rel="noopener nofollow" target="_blank">Implementations</a> of many ML algorithms. Good to search for decision trees and k-NN.</li><li name="bef3" id="bef3" class="graf graf--li graf-after--li graf--trailing">And many others, feel free to share in comments.</li></ul></div></div></section><section name="a08a" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="541b" id="541b" class="graf graf--p graf--leading graf--trailing"><em class="markup--em markup--p-em">Author: </em><a href="https://www.linkedin.com/in/festline/" data-href="https://www.linkedin.com/in/festline/" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Yury Kashnitsky</em></a><em class="markup--em markup--p-em">, Data Scientist at Mail.Ru Group. Translated and edited by </em><a href="https://www.linkedin.com/in/christinabutsko/" data-href="https://www.linkedin.com/in/christinabutsko/" class="markup--anchor markup--p-anchor" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Christina Butsko</em></a><em class="markup--em markup--p-em">, Gleb Filatov, and </em><a href="https://www.linkedin.com/in/yuanyuanpao/" data-href="https://www.linkedin.com/in/yuanyuanpao/" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener nofollow noopener" target="_blank"><em class="markup--em markup--p-em">Yuanyuan Pao</em></a><em class="markup--em markup--p-em">.</em></p></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link" href="https://medium.com/tag/machine-learning?source=post" data-action-source="post">Machine Learning</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/classification?source=post" data-action-source="post">Classification</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/decision-tree?source=post" data-action-source="post">Decision Tree</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/scikit-learn?source=post" data-action-source="post">Scikit Learn</a></li><li><a class="link u-baseColor--link" href="https://medium.com/tag/education?source=post" data-action-source="post">Education</a></li></ul></div></div></div><section class="uiScale uiScale-ui--small uiScale-caption--regular u-borderTopLightest u-marginTop10 u-paddingTop20"><div class="ui-h3 u-textColorDarker u-fontSize22">One clap, two clap, three clap, forty?</div><p class="ui-body u-marginBottom20 u-textColorDark u-fontSize16">By clapping more or less, you can signal to us which stories really stand out.</p></section><div class="postActions js-postActionsFooter"><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter u-width60" data-post-id="8613c6b6d2cd" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----8613c6b6d2cd---------------------clap_footer"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboardingcollection" data-action="multivote" data-action-value="8613c6b6d2cd" data-action-type="long-press" data-action-source="post_actions_footer-----8613c6b6d2cd---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="8613c6b6d2cd"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft10"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="8613c6b6d2cd">3.8K</button></span></div></div><div class="buttonSet u-flex0"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="scroll-to-responses">2</button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="8613c6b6d2cd" data-action-source="post_actions_footer"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon js-moreActionsButton" title="More actions" aria-label="More actions" data-action="more-actions"><span class="svgIcon svgIcon--more svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="-480.5 272.5 21 21"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="5572ec600139"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton" data-action="toggle-block-user" data-action-value="5572ec600139" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton" data-action="toggle-subscribe-user" data-action-value="5572ec600139" data-action-source="footer_card-5572ec600139-------------------------follow_footer" data-subscribe-source="footer_card" data-follow-context-entity-id="8613c6b6d2cd"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar" href="https://medium.com/@yurykashnitskiy?source=footer_card" title="Go to the profile of Yury Kashnitskiy" aria-label="Go to the profile of Yury Kashnitskiy" data-action-source="footer_card" data-user-id="5572ec600139" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_JtleyoApNKIjJU5F" class="avatar-image avatar-image--small" alt="Go to the profile of Yury Kashnitskiy"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://medium.com/@yurykashnitskiy" property="cc:attributionName" title="Go to the profile of Yury Kashnitskiy" aria-label="Go to the profile of Yury Kashnitskiy" rel="author cc:attributionUrl" data-user-id="5572ec600139" dir="auto">Yury Kashnitskiy</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Data Scientist at Mail.Ru Group</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--filled button--small button--withChrome u-accentColor--buttonNormal button--withIcon button--withSvgIcon button--withIconRight button--withIconAndLabel js-relationshipButton" data-action="show-more-collection-actions" data-collection-id="b76595fb6cfc"><span class="button-label  js-buttonLabel">Following</span><span class="svgIcon svgIcon--arrowDown svgIcon--21px"><svg class="svgIcon-use" width="21" height="21" viewBox="0 0 21 21"><path d="M4 7.331l6.032 6.67.495.547.495-.547 5.973-6.603-.989-.895-5.974 6.603h.99l-6.033-6.67z" fill-rule="evenodd"></path></svg></span></button></div><div class="u-tableCell "><a class="link u-baseColor--link avatar avatar--roundedRectangle" href="https://medium.com/open-machine-learning-course?source=footer_card" title="Go to Open Machine Learning Course" aria-label="Go to Open Machine Learning Course" data-action-source="footer_card"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_b5pfoCQz20rjh79OPSQWQg.jpeg" class="avatar-image u-size60x60" alt="Open Machine Learning Course"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal" href="https://medium.com/open-machine-learning-course?source=footer_card" rel="collection" data-action-source="footer_card">Open Machine Learning Course</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">A series of articles on basics of Machine Learning. Each article is followed by an assignment with a deadline. Several Kaggle Inclass competitions are held throughout the course.</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements"><div class="streamItem streamItem--placementCardGrid js-streamItem"><div class="u-clearfix u-backgroundGrayLightest"><div class="row u-marginAuto u-maxWidth1000 u-paddingTop30 u-paddingBottom40"><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-sizeFullWidth u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="d2da3d09f664" data-source="placement_card_footer_grid---------0-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://medium.com/@yashwant140393/the-3-pillars-of-binary-classification-accuracy-precision-recall-d2da3d09f664?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-sizeFullWidth u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/800/240/1*rzHGTRd9wR_aaoUBnvq_nw.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://medium.com/@yashwant140393/the-3-pillars-of-binary-classification-accuracy-precision-recall-d2da3d09f664?source=placement_card_footer_grid---------0-43" data-action-source="placement_card_footer_grid---------0-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Also tagged Classification</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">The 3 Pillars of Binary Classification: Accuracy, Precision &amp; Recall</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@yashwant140393" data-action="show-user-card" data-action-value="935b9744dd8" data-action-type="hover" data-user-id="935b9744dd8" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_RWlq-v7g6i8jPrNz" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Yashwant Jankay"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://medium.com/@yashwant140393?source=placement_card_footer_grid---------0-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------0-43" data-action-value="935b9744dd8" data-action-type="hover" data-user-id="935b9744dd8" dir="auto">Yashwant Jankay</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal" title="5 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="d2da3d09f664" data-is-label-padded="true" data-source="placement_card_footer_grid-----d2da3d09f664----0-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="d2da3d09f664" data-action-type="long-press" data-action-source="placement_card_footer_grid-----d2da3d09f664----0-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="d2da3d09f664">41</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="d2da3d09f664" data-action-source="placement_card_footer_grid-----d2da3d09f664----0-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-sizeFullWidth u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="d9ae088de117" data-source="placement_card_footer_grid---------1-43" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://blog.quiltdata.com/reproducible-machine-learning-with-jupyter-and-quilt-d9ae088de117?source=placement_card_footer_grid---------1-43" data-action-source="placement_card_footer_grid---------1-43"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-sizeFullWidth u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/800/240/1*ARmkE4ri8QzL_dvbuph0Qw.jpeg&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://blog.quiltdata.com/reproducible-machine-learning-with-jupyter-and-quilt-d9ae088de117?source=placement_card_footer_grid---------1-43" data-action-source="placement_card_footer_grid---------1-43"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Also tagged Scikit Learn</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">Reproducible machine learning with Jupyter and Quilt</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://blog.quiltdata.com/@akarve" data-action="show-user-card" data-action-value="6c9d5b577198" data-action-type="hover" data-user-id="6c9d5b577198" data-collection-slug="quilt" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_tQ0u1W4Bbt0fVl3kz_38hw.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Aneesh Karve"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://blog.quiltdata.com/@akarve?source=placement_card_footer_grid---------1-43" data-action="show-user-card" data-action-source="placement_card_footer_grid---------1-43" data-action-value="6c9d5b577198" data-action-type="hover" data-user-id="6c9d5b577198" data-collection-slug="quilt" dir="auto">Aneesh Karve</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal" title="1 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="d9ae088de117" data-is-label-padded="true" data-source="placement_card_footer_grid-----d9ae088de117----1-43----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="d9ae088de117" data-action-type="long-press" data-action-source="placement_card_footer_grid-----d9ae088de117----1-43----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="d9ae088de117">67</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="d9ae088de117" data-action-source="placement_card_footer_grid-----d9ae088de117----1-43----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div><div class="col u-padding8 u-xs-size12of12 u-size4of12"><div class="uiScale uiScale-ui--small uiScale-caption--regular u-height280 u-sizeFullWidth u-backgroundWhite u-borderCardBorder u-boxShadow u-borderBox u-borderRadius4 js-trackedPost" data-post-id="5582fb7e0a9c" data-source="placement_card_footer_grid---------2-60" data-tracking-context="placement" data-scroll="native"><a class="link link--noUnderline u-baseColor--link" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="u-backgroundCover u-backgroundColorGrayLight u-height100 u-sizeFullWidth u-borderBottomLight u-borderRadiusTop4" style="background-image: url(&quot;https://cdn-images-1.medium.com/fit/c/800/240/1*V3JWBvxB92Uo116Bpxa3Tw.png&quot;); background-position: 50% 50% !important;"></div></a><div class="u-padding15 u-borderBox u-flexColumn u-height180"><a class="link link--noUnderline u-baseColor--link u-flex1" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c?source=placement_card_footer_grid---------2-60" data-action-source="placement_card_footer_grid---------2-60"><div class="uiScale uiScale-ui--regular uiScale-caption--small u-textColorNormal u-marginBottom7">Related reads</div><div class="ui-h3 ui-clamp2 u-textColorDarkest u-contentSansBold u-fontSize24 u-maxHeight2LineHeightTighter u-lineClamp2 u-textOverflowEllipsis u-letterSpacingTight u-paddingBottom2">A One-Stop Shop for Principal Component Analysis</div></a><div class="u-paddingBottom10 u-flex0 u-flexCenter"><div class="u-flex1 u-minWidth0 u-marginRight10"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://towardsdatascience.com/@matthew.w.brems" data-action="show-user-card" data-action-value="55680478461" data-action-type="hover" data-user-id="55680478461" data-collection-slug="towards-data-science" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/1_dZnpWXk6kH0WKDYn8bkjwQ.jpeg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Matt Brems"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--darker" href="https://towardsdatascience.com/@matthew.w.brems?source=placement_card_footer_grid---------2-60" data-action="show-user-card" data-action-source="placement_card_footer_grid---------2-60" data-action-value="55680478461" data-action-type="hover" data-user-id="55680478461" data-collection-slug="towards-data-science" dir="auto">Matt Brems</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><span class="readingTime u-textColorNormal" title="15 min read"></span></div></div></div></div><div class="u-flex0 u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="5582fb7e0a9c" data-is-label-padded="true" data-source="placement_card_footer_grid-----5582fb7e0a9c----2-60----------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="5582fb7e0a9c" data-action-type="long-press" data-action-source="placement_card_footer_grid-----5582fb7e0a9c----2-60----------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents u-marginLeft4" data-action="show-recommends" data-action-value="5582fb7e0a9c">1.1K</button></span></div></div><div class="u-height20 u-borderRightLighter u-inlineBlock u-relative u-marginRight10 u-marginLeft12"></div><div class="buttonSet"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="5582fb7e0a9c" data-action-source="placement_card_footer_grid-----5582fb7e0a9c----2-60----------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button></div></div></div></div></div></div></div></div></div></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper" data-action-scope="_actionscope_5"><div class="container u-maxWidth740"><div class="responsesStreamWrapper u-maxWidth640 js-responsesStreamWrapper"><div class="container responsesStream-title u-paddingTop15"><div class="row"><header class="heading"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title heading-title--semibold">Responses</span></div></div></header></div></div><div class="responsesStream-editor cardChromeless u-marginBottom20 u-paddingLeft20 u-paddingRight20 js-responsesStreamEditor"><div class="inlineNewPostControl js-inlineNewPostControl" data-action-scope="_actionscope_9"><div class="inlineEditor is-collapsed is-postEditMode js-inlineEditor" data-action="focus-editor"><div class="u-paddingTop20 js-block js-inlineEditorContent"><div class="inlineEditor-header"><div class="inlineEditor-avatar u-paddingRight20"><div class="avatar u-inline"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_P-t7h_lDOjJiWVxI(1)" class="avatar-image u-size36x36 u-xs-size32x32" alt="陈辰"></div></div><div class="inlineEditor-headerContent"><div class="inlineEditor-placeholder js-inlineEditorPrompt">Write a response…</div><div class="inlineEditor-author u-accentColor--textNormal">陈辰</div></div></div></div></div></div></div><div class="responsesStream js-responsesStream"><div class="streamItem streamItem--conversation js-streamItem" data-action-scope="_actionscope_6"><div class="streamItemConversation"><div class="u-marginLeft20"><div class="streamItemConversation-divider"></div><header class="heading heading--light heading--simple"><div class="u-clearfix"><div class="heading-content u-floatLeft"><span class="heading-title">Conversation between <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@weakish" data-action="show-user-card" data-action-value="c2d21d069ac" data-action-type="hover" data-user-id="c2d21d069ac" dir="auto">Jakukyo Friel</a> and <a class="link link--accent u-accentColor--textNormal u-baseColor--link" href="https://medium.com/@yurykashnitskiy" data-action="show-user-card" data-action-value="5572ec600139" data-action-type="hover" data-user-id="5572ec600139" dir="auto">Yury Kashnitskiy</a>.</span></div></div></header></div><div class="streamItemConversation-inner cardChromeless"><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="8c2ea26d224d" data-source="responses---------0----------------" data-action-scope="_actionscope_7" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@weakish" data-action="show-user-card" data-action-value="c2d21d069ac" data-action-type="hover" data-user-id="c2d21d069ac" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_W79jtLNuG1j26Z8C.jpg" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Jakukyo Friel"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@weakish?source=responses---------0----------------" data-action="show-user-card" data-action-source="responses---------0----------------" data-action-value="c2d21d069ac" data-action-type="hover" data-user-id="c2d21d069ac" dir="auto">Jakukyo Friel</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@weakish/just-finished-the-chinese-translation-https-www-jqr-com-article-000139-8c2ea26d224d?source=responses---------0----------------" data-action="open-post" data-action-value="https://medium.com/@weakish/just-finished-the-chinese-translation-https-www-jqr-com-article-000139-8c2ea26d224d?source=responses---------0----------------" data-action-source="preview-listing"><time datetime="2018-04-09T07:54:52.040Z">Apr 9</time></a><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="2 min read"></span></div></div></div></div></div><div class="js-inlineExpandBody"><a class="" href="https://medium.com/@weakish/just-finished-the-chinese-translation-https-www-jqr-com-article-000139-8c2ea26d224d?source=responses---------0----------------" data-action="expand-inline"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="1292" id="1292" class="graf graf--p graf--leading">Just finished the Chinese translation: <span class="markup--anchor markup--p-anchor" data-action="open-inner-link" data-action-value="https://www.jqr.com/article/000139">https://www.jqr.com/article/000139</span></p><p name="7e44" id="7e44" class="graf graf--p graf-after--p">Possible typos:</p><p name="d683" id="d683" class="graf graf--p graf-after--p">In the section “Article outline”:</p><p name="7e1b" id="7e1b" class="graf graf--p graf-after--p graf--trailing">&gt; 5. Application Examples and Complex Cases<br>&gt; — Decision trees and nearest neighbors method in a customer churn prediction task</p></div></div></section></div></a></div><div class="postArticle-readMore"><button class="button button--smaller button--link u-baseColor--buttonNormal" data-action="expand-inline">Read more…</button></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="8c2ea26d224d" data-is-flush-left="true" data-source="listing-----8c2ea26d224d---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="8c2ea26d224d" data-action-type="long-press" data-action-source="listing-----8c2ea26d224d---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents" data-action="show-recommends" data-action-value="8c2ea26d224d">5</button></span></div></div><div class="buttonSet u-floatRight"><a class="button button--chromeless u-baseColor--buttonNormal" href="https://medium.com/@weakish/just-finished-the-chinese-translation-https-www-jqr-com-article-000139-8c2ea26d224d?source=responses---------0----------------#--responses" data-action-source="responses---------0----------------">1 response</a><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="8c2ea26d224d" data-action-source="listing-----8c2ea26d224d---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="8c2ea26d224d"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19" viewBox="0 0 19 19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="streamItemConversationItem streamItemConversationItem--preview"><div class="postArticle js-postArticle js-trackedPost postArticle--short" data-post-id="2ca793877fe1" data-source="responses---------0----------------" data-action-scope="_actionscope_8" data-scroll="native"><div class="u-clearfix u-marginBottom15 u-paddingTop5"><div class="postMetaInline u-floatLeft"><div class="u-flexCenter"><div class="postMetaInline-avatar u-flex0"><a class="link u-baseColor--link avatar" href="https://medium.com/@yurykashnitskiy" data-action="show-user-card" data-action-value="5572ec600139" data-action-type="hover" data-user-id="5572ec600139" dir="auto"><img src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/0_JtleyoApNKIjJU5F(1)" class="avatar-image u-size36x36 u-xs-size32x32" alt="Go to the profile of Yury Kashnitskiy"></a></div><div class="postMetaInline postMetaInline-authorLockup ui-captionStrong u-flex1 u-noWrapWithEllipsis"><a class="ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken" href="https://medium.com/@yurykashnitskiy?source=responses---------0----------------" data-action="show-user-card" data-action-source="responses---------0----------------" data-action-value="5572ec600139" data-action-type="hover" data-user-id="5572ec600139" dir="auto">Yury Kashnitskiy</a><div class="ui-caption u-fontSize12 u-baseColor--textNormal u-textColorNormal js-postMetaInlineSupplemental"><a class="link link--darken" href="https://medium.com/@yurykashnitskiy/thank-you-fixed-everything-2ca793877fe1?source=responses---------0----------------" data-action="open-post" data-action-value="https://medium.com/@yurykashnitskiy/thank-you-fixed-everything-2ca793877fe1?source=responses---------0----------------" data-action-source="preview-listing"><time datetime="2018-04-09T09:50:19.972Z">Apr 9</time></a></div></div></div></div></div><div><a class="" href="https://medium.com/@yurykashnitskiy/thank-you-fixed-everything-2ca793877fe1?source=responses---------0----------------"><div class="postArticle-content js-postField"><section class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="5b37" id="5b37" class="graf graf--p graf--leading graf--trailing">Thank you! Fixed everything.</p></div></div></section></div></a></div><div class="u-clearfix u-paddingTop10"><div class="u-floatLeft"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="2ca793877fe1" data-is-flush-left="true" data-source="listing-----2ca793877fe1---------------------clap_preview"><div class="u-relative u-foreground"><button class="button button--primary button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="2ca793877fe1" data-action-type="long-press" data-action-source="listing-----2ca793877fe1---------------------clap_preview" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.739 0l.761 2.966L13.261 0z"></path><path d="M14.815 3.776l1.84-2.551-1.43-.471z"></path><path d="M8.378 1.224l1.84 2.551L9.81.753z"></path><path d="M20.382 21.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L4.11 14.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L6.43 8.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L18.628 14c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM10.99 5.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--25px is-flushLeft"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.738 0l.762 2.966L13.262 0z"></path><path d="M16.634 1.224l-1.432-.47-.408 3.022z"></path><path d="M9.79.754l-1.431.47 1.84 2.552z"></path><path d="M22.472 13.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M12.58 9.887c-.156-.83.096-1.569.692-2.142L10.78 5.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M15.812 9.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L7.2 6.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L5.046 8.54 3.802 7.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394L3.647 9.93l4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L2.89 10.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C19.74 19.8 20.271 17 18.62 13.982L15.812 9.04z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents" data-action="show-recommends" data-action-value="2ca793877fe1">1</button></span></div></div><div class="buttonSet u-floatRight"><button class="button button--chromeless is-touchIconFadeInPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="2ca793877fe1" data-action-source="listing-----2ca793877fe1---------------------bookmark_preview"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126a.508.508 0 0 0 .708-.03.5.5 0 0 0 .118-.285H19V6zm-6.838 9.97L7 19.636V6c0-.55.45-1 1-1h9c.55 0 1 .45 1 1v13.637l-5.162-3.668a.49.49 0 0 0-.676 0z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19 6c0-1.1-.9-2-2-2H8c-1.1 0-2 .9-2 2v14.66h.012c.01.103.045.204.12.285a.5.5 0 0 0 .706.03L12.5 16.85l5.662 4.126c.205.183.52.17.708-.03a.5.5 0 0 0 .118-.285H19V6z"></path></svg></span></span></button><button class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon js-postActionsButton" data-action="post-actions" data-action-value="2ca793877fe1"><span class="svgIcon svgIcon--arrowDown svgIcon--19px is-flushRight"><svg class="svgIcon-use" width="19" height="19" viewBox="0 0 19 19"><path d="M3.9 6.772l5.205 5.756.427.472.427-.472 5.155-5.698-.854-.772-4.728 5.254L4.753 6z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></div></div></div></div><div class="container js-showOtherResponses"><div class="row"><button class="button button--primary button--withChrome u-accentColor--buttonNormal responsesStream-showOtherResponses cardChromeless u-sizeFullWidth u-marginVertical20 u-heightAuto" data-action="show-other-responses">Show all responses</button></div></div><div class="responsesStream js-responsesStreamOther"></div></div></div></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1000 js-postLeftSidebar metabar-sibling"><div class="u-foreground u-top0 u-sm-hide u-marginLeftNegative12 js-postShareWidget u-transition--fadeIn300 u-fixed" data-scroll="fixed" style="transform: translateY(150px);"><ul><li class="u-textAlignCenter u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexColumn u-marginBottom10 u-width60" data-post-id="8613c6b6d2cd" data-is-icon-29px="true" data-is-vertical="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_share_widget-----8613c6b6d2cd---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal" data-action="multivote" data-action-value="8613c6b6d2cd" data-action-type="long-press" data-action-source="post_share_widget-----8613c6b6d2cd---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33"><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"></path><path d="M13.285.48l-1.916.881 2.37 2.837z"></path><path d="M21.719 1.361L19.79.501l-.44 3.697z"></path><path d="M16.502 3.298L15.481 0h2.043z"></path></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight" data-action="multivote-undo" data-action-value="8613c6b6d2cd"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"></path></svg></span></button></div></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-flexOrderNegative1 u-height20 u-marginBottom7"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-block u-marginAuto" data-action="show-recommends" data-action-value="8613c6b6d2cd">3.8K</button></span></div></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_share_widget"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_share_widget"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="8613c6b6d2cd" data-action-source="post_share_widget-----8613c6b6d2cd---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></li></ul></div></aside><div class="postActionsBar uiScale uiScale-ui--regular uiScale-caption--regular js-postActionsBar metabar-sibling is-visible"><div class="postActionsBar-container container"><div class="postActionsBar-content row u-boxShadowNormal js-postActionsBarContent"><div class="postActions"><div class="u-height44 u-maxWidth1250 u-marginAuto u-relative"><div class="u-absolute u-right0 u-marginRight20 u-sizeFullHeight u-flexCenter"><div class="u-floatLeft"><div class="u-floatRight buttonSet"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon" data-action="respond" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal" data-action="scroll-to-responses">2</button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide" title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide" title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg></span></button><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show" title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_bar"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"></path></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton" title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="add-to-bookmarks" data-action-value="8613c6b6d2cd" data-action-source="post_actions_bar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"></path></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"></path></svg></span></span></button></div></div><div class="u-xs-hide u-width255 u-height44 u-floatLeft js-readNextMetabarRight"><div class="streamItem streamItem--placementCardMetabar js-streamItem"><div class="readNextPromo readNextPromo--chromeless"><div class="buttonSet-separator u-floatLeft"></div><div class="js-trackedPost" data-post-id="c751538131ac" data-source="read_next_metabar----------41" data-tracking-context="placement" data-scroll="fixed"><a class="link link--noUnderline u-baseColor--link u-block" href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-10-gradient-boosting-c751538131ac?source=read_next_metabar----------41"><div class="uiScale uiScale-ui--small uiScale-caption--small readNextPromo-postInfo"><div class="ui-caption u-marginTop5 readNextPromo-callToAction u-accentColor--textNormal u-uiTextBold">Next story</div><h4 class="ui-h4 readNextPromo-postTitle u-block u-noWrapWithEllipsis u-contentSansBold">Open Machine Learning Course. Topic 10. Gradient Boosting</h4></div></a></div></div></div></div></div><div class="container u-maxWidth740 u-sizeFullHeight u-flexCenter"><div class="buttonSet"><div class="multirecommend js-actionMultirecommend u-flexCenter" data-post-id="8613c6b6d2cd" data-is-icon-29px="true" data-has-recommend-list="true" data-source="post_actions_bar-----8613c6b6d2cd---------------------clap_footer"><div class="u-relative u-foreground"><button class="button button--primary button--large button--chromeless u-accentColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton" data-action="multivote" data-action-value="8613c6b6d2cd" data-action-type="long-press" data-action-source="post_actions_bar-----8613c6b6d2cd---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><g fill-rule="evenodd"><path d="M13.739 1l.761 2.966L15.261 1z"></path><path d="M16.815 4.776l1.84-2.551-1.43-.471z"></path><path d="M10.378 2.224l1.84 2.551-.408-3.022z"></path><path d="M22.382 22.622c-1.04 1.04-2.115 1.507-3.166 1.608.168-.14.332-.29.492-.45 2.885-2.886 3.456-5.982 1.69-9.211l-1.101-1.937-.955-2.02c-.315-.676-.235-1.185.245-1.556a.836.836 0 0 1 .66-.16c.342.056.66.28.879.605l2.856 5.023c1.179 1.962 1.379 5.119-1.6 8.098m-13.29-.528l-5.02-5.02a1 1 0 0 1 .707-1.701c.255 0 .512.098.707.292l2.607 2.607a.442.442 0 0 0 .624-.624L6.11 15.04l-1.75-1.75a.998.998 0 1 1 1.41-1.413l4.154 4.156a.44.44 0 0 0 .624 0 .44.44 0 0 0 0-.624l-4.152-4.153-1.172-1.171a.998.998 0 0 1 0-1.41 1.018 1.018 0 0 1 1.41 0l1.172 1.17 4.153 4.152a.437.437 0 0 0 .624 0 .442.442 0 0 0 0-.624L8.43 9.222a.988.988 0 0 1-.291-.705.99.99 0 0 1 .29-.706 1 1 0 0 1 1.412 0l6.992 6.993a.443.443 0 0 0 .71-.501l-1.35-2.856c-.315-.676-.235-1.185.246-1.557a.85.85 0 0 1 .66-.16c.342.056.659.28.879.606L20.628 15c1.573 2.876 1.067 5.545-1.544 8.156-1.396 1.397-3.144 1.966-5.063 1.652-1.713-.286-3.463-1.248-4.928-2.714zM12.99 6.976l2.562 2.562c-.497.607-.563 1.414-.155 2.284l.265.562-4.257-4.257a.98.98 0 0 1-.117-.445c0-.267.104-.517.292-.706a1.023 1.023 0 0 1 1.41 0zm8.887 2.06c-.375-.557-.902-.916-1.486-1.011a1.738 1.738 0 0 0-1.342.332c-.376.29-.61.656-.712 1.065a2.1 2.1 0 0 0-1.095-.562 1.776 1.776 0 0 0-.992.128l-2.636-2.636a1.883 1.883 0 0 0-2.658 0 1.862 1.862 0 0 0-.478.847 1.886 1.886 0 0 0-2.671-.012 1.867 1.867 0 0 0-.503.909c-.754-.754-1.992-.754-2.703-.044a1.881 1.881 0 0 0 0 2.658c-.288.12-.605.288-.864.547a1.884 1.884 0 0 0 0 2.659l.624.622a1.879 1.879 0 0 0-.91 3.16l5.019 5.02c1.595 1.594 3.515 2.645 5.408 2.959a7.16 7.16 0 0 0 1.173.098c1.026 0 1.997-.24 2.892-.7.279.04.555.065.828.065 1.53 0 2.969-.628 4.236-1.894 3.338-3.338 3.083-6.928 1.738-9.166l-2.868-5.043z"></path></g></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><g fill-rule="evenodd"><path d="M13.738 1l.762 2.966L15.262 1z"></path><path d="M18.634 2.224l-1.432-.47-.408 3.022z"></path><path d="M11.79 1.754l-1.431.47 1.84 2.552z"></path><path d="M24.472 14.307l-3.023-5.32c-.287-.426-.689-.705-1.123-.776a1.16 1.16 0 0 0-.911.221c-.297.231-.474.515-.535.84.017.022.036.04.053.063l2.843 5.001c1.95 3.564 1.328 6.973-1.843 10.144a8.46 8.46 0 0 1-.549.501c1.205-.156 2.328-.737 3.351-1.76 3.268-3.268 3.041-6.749 1.737-8.914"></path><path d="M14.58 10.887c-.156-.83.096-1.569.692-2.142L12.78 6.252c-.5-.504-1.378-.504-1.879 0-.178.18-.273.4-.329.63l4.008 4.005z"></path><path d="M17.812 10.04c-.218-.323-.539-.55-.88-.606a.814.814 0 0 0-.644.153c-.176.137-.713.553-.24 1.566l1.43 3.025a.539.539 0 1 1-.868.612L9.2 7.378a.986.986 0 1 0-1.395 1.395l4.401 4.403a.538.538 0 1 1-.762.762L7.046 9.54 5.802 8.295a.99.99 0 0 0-1.396 0 .981.981 0 0 0 0 1.394l1.241 1.241 4.402 4.403a.537.537 0 0 1 0 .761.535.535 0 0 1-.762 0L4.89 11.696a.992.992 0 0 0-1.399-.003.983.983 0 0 0 0 1.395l1.855 1.854 2.763 2.765a.538.538 0 0 1-.76.761l-2.765-2.764a.982.982 0 0 0-1.395 0 .989.989 0 0 0 0 1.395l5.32 5.32c3.371 3.372 6.64 4.977 10.49 1.126C21.74 20.8 22.271 18 20.62 14.982l-2.809-4.942z"></path></g></svg></span></span></button></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton" data-action="show-recommends" data-action-value="8613c6b6d2cd">3.8K</button></span></div></div></div></div></div></div></div></div><style class="js-collectionStyle metabar-sibling">
.u-accentColor--borderLight {border-color: #02B875 !important;}
.u-accentColor--borderNormal {border-color: #02B875 !important;}
.u-accentColor--borderDark {border-color: #1C9963 !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #02B875 !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #02B875 !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #1C9963 !important;}
.u-accentColor--textNormal {color: #1C9963 !important;}
.u-accentColor--hoverTextNormal:hover {color: #1C9963 !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #1C9963 !important;}
.u-accentColor--textDark {color: #1C9963 !important;}
.u-accentColor--backgroundLight {background-color: #02B875 !important;}
.u-accentColor--backgroundNormal {background-color: #02B875 !important;}
.u-accentColor--backgroundDark {background-color: #1C9963 !important;}
.u-accentColor--buttonDark {border-color: #1C9963 !important; color: #1C9963 !important;}
.u-accentColor--buttonDark:hover {border-color: #1C9963 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #1C9963 !important; fill: #1C9963 !important;}
.u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #02B875 !important; color: #1C9963 !important;}
.u-accentColor--buttonNormal:hover {border-color: #1C9963 !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #02B875 !important; fill: #02B875 !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #1C9963 !important; border-color: #1C9963 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #02B875 !important; border-color: #02B875 !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #1C9963 !important;}
.u-accentColor--highlightFaint {background-color: rgba(233, 253, 240, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(125, 255, 179, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(233, 253, 240, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(233, 253, 240, 1), rgba(233, 253, 240, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(173, 255, 207, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(173, 255, 207, 1), rgba(173, 255, 207, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(125, 255, 179, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(125, 255, 179, 1), rgba(125, 255, 179, 1));}.u-baseColor--iconNormal.avatar-halo {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}</style><style class="js-collectionStyleConstant metabar-sibling">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDarker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(255, 255, 255, 0.4) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(255, 255, 255, 0.4980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-h1,.u-imageSpectrum  .ui-h2,.u-imageSpectrum  .ui-h3,.u-imageSpectrum  .ui-h4,.u-imageSpectrum  .ui-brand1,.u-imageSpectrum  .ui-brand2,.u-imageSpectrum  .ui-captionStrong {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-body,.u-imageSpectrum  .ui-caps {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-summary,.u-imageSpectrum  .ui-caption {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDarker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(0, 0, 0, 0.4) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-h1,.u-resetSpectrum  .ui-h2,.u-resetSpectrum  .ui-h3,.u-resetSpectrum  .ui-h4,.u-resetSpectrum  .ui-brand1,.u-resetSpectrum  .ui-brand2,.u-resetSpectrum  .ui-captionStrong {color: rgba(0, 0, 0, 0.8) !important; fill: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum  .ui-body,.u-resetSpectrum  .ui-caps {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-summary,.u-resetSpectrum  .ui-caption {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style><div class="highlightMenu metabar-sibling" data-action-scope="_actionscope_3"><div class="highlightMenu-inner"><div class="buttonSet"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu u-accentColor--highlightStrong js-highlightMenuQuoteButton" data-action="quote" data-action-source="quote_menu--------------------------highlight_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--highlighter svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M13.7 15.964l5.204-9.387-4.726-2.62-5.204 9.387 4.726 2.62zm-.493.885l-1.313 2.37-1.252.54-.702 1.263-3.796-.865 1.228-2.213-.202-1.35 1.314-2.37 4.722 2.616z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="quote-respond" data-action-source="quote_menu--------------------------respond_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--responseFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M19.074 21.117c-1.244 0-2.432-.37-3.532-1.096a7.792 7.792 0 0 1-.703-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.662 0 8.457 3.5 8.457 7.803 0 2.058-.85 3.984-2.403 5.448.023.17.06.35.118.55.192.69.537 1.38 1.026 2.04.15.21.172.48.058.7a.686.686 0 0 1-.613.38h-.03z" fill-rule="evenodd"></path></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="twitter" data-action-source="quote_menu" data-skip-onboarding="true"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"></path></svg></span></button><div class="buttonSet-separator"></div><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon button--highlightMenu" data-action="highlight" data-action-source="quote_menu--------------------------privatenote_text" data-skip-onboarding="true"><span class="svgIcon svgIcon--privatenoteFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M17.662 4.552H7.346A4.36 4.36 0 0 0 3 8.898v5.685c0 2.168 1.614 3.962 3.697 4.28v2.77c0 .303.35.476.59.29l3.904-2.994h6.48c2.39 0 4.35-1.96 4.35-4.35V8.9c0-2.39-1.95-4.346-4.34-4.346zM16 14.31a.99.99 0 0 1-1.003.99h-4.994C9.45 15.3 9 14.85 9 14.31v-3.02a.99.99 0 0 1 1-.99v-.782a2.5 2.5 0 0 1 2.5-2.51c1.38 0 2.5 1.13 2.5 2.51v.782c.552.002 1 .452 1 .99v3.02z"></path><path d="M14 9.81c0-.832-.674-1.68-1.5-1.68-.833 0-1.5.84-1.5 1.68v.49h3v-.49z"></path></g></svg></span></button></div></div><div class="highlightMenu-arrowClip"><span class="highlightMenu-arrow"></span></div></div></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"33632-059e361","currentUser":{"userId":"a988255767e5","username":"0109chenchen","name":"陈辰","email":"0109chenchen@gmail.com","imageId":"0*P-t7h_lDOjJiWVxI.","twitterScreenName":"0109chenchen","createdAt":1463006784997,"isVerified":true,"subscriberEmail":"","onboardingStatus":1,"googleAccountId":"104969029311726520174","googleEmail":"0109chenchen@gmail.com","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":true,"hightowerLastLockedAt":0},"currentUserHasUnverifiedEmail":false,"isAuthenticated":true,"isCurrentUserVerified":true,"language":"zh-cn","mediumTwitterScreenName":"medium","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.sSNLBL0zi5BWVp4gJcuzqw.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.S7356n6LBseGGEx0Avxzng.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.R4Z1Bl6LohSM1oRSQ_gbJA.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.lFcVOa9Ax8v0ud745p9xIA.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.OM_Lvs_caTF1Fofc14d0Ng.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.CzX6ttriZ_q3KYPaiNXuzw.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.AxpoSoOIvhoAGzuOoj5orQ.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.BVQI0_KVa8XfbTvOLOVFsQ.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.oGj7bY7Ny-SWzMrYzccKeQ.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.mxIgO8Ehuyccn0jcgf68wg.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.VKeSHsc9Zb1ckcynwkC7KA.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1525760893898:9a571d37a883","useragent":{"browser":"chrome","family":"chrome","os":"mac","version":66,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":5,"isSearchBot":false,"isSyndicationBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv","signup_services":"twitter,facebook,google,email,google-fastidv","android_rating_prompt_recommend_threshold":5,"google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"enable_export_members":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","enable_sms":true,"disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"disable_followed_tag_fan_out":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"enable_sequence_carousel":true,"enable_multirecommends":true,"enable_post_monger_v2":true,"enable_post_monger_v3":true,"enable_hightower_editor_copy_v2":true,"enable_user_post_metering":true,"max_premium_content_per_user_under_metering":3,"tag_intercom_user_on_metering_count":3,"enable_topic_writer_onboarding":true,"enable_strong_graph_chp_reorder":true,"enable_unsplash_images":true,"enable_top_stories_for_you":true,"enable_ios_member_post_labeling":true,"enable_lite_profile":true,"enable_li_search_collection":true,"enable_homepage_remodel":true,"enable_signin_wall_custom_domain":true,"enable_standalone_profile_edit_page":true,"enable_standalone_user_follow_pages":true,"enable_post_footer_copy":true,"app_download_email_template":"control","enable_email_sign_in_captcha":true,"enable_topic_lifecycle_email":true,"enable_curation_post_locking":true,"ios_hide_avatars_on_home":true,"raise_editors_picks_digest":"control","android_disable_author_avatars":true,"enable_persistent_user_id_for_dnt":true,"enable_truncated_rss_for_tags_and_topics":true,"enable_ios_related_reads_api_change":true,"enable_ios_related_reads_ui_large":true,"enable_ios_responses_collapsed":true},"xsrfToken":"mVkkZYUx0tcp","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"},{"id":10,"url":"https://glyph.medium.com/css/elv8.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"48fc2dc11c632908\",\"ot-tracer-traceid\":\"65c7382f721add87\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email","user_friends"],"connect":["public_profile","email","user_friends"],"login":["public_profile","email","user_friends"],"share":["public_profile","email","user_friends","publish_actions"]}},"mailingListArchiveUploadSizeMb":2,"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"goldfinchUrl":"https://goldfinch.medium.com","buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":1,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"li_post_page","type":0,"url":"www.calendly.com"},{"promptId":"li_home_page","type":1,"url":"mediumuserfeedback.typeform.com/to/GcFjEO"},{"promptId":"li_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","paypalClientMode":"production","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"CA"}
// ]]></script><script charset="UTF-8" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/main-base.bundle.sSNLBL0zi5BWVp4gJcuzqw.js" async=""></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"8613c6b6d2cd","versionId":"2f4e00167733","creatorId":"5572ec600139","creator":{"userId":"5572ec600139","name":"Yury Kashnitskiy","username":"yurykashnitskiy","createdAt":1490200873092,"lastPostCreatedAt":1525724892687,"imageId":"0*JtleyoApNKIjJU5F.","backgroundImageId":"","bio":"Data Scientist at Mail.Ru Group","twitterScreenName":"","socialStats":{"userId":"5572ec600139","usersFollowedCount":33,"usersFollowedByCount":650,"type":"SocialStats"},"social":{"userId":"a988255767e5","targetUserId":"5572ec600139","type":"Social"},"facebookAccountId":"1414603208563160","allowNotes":1,"isNsfw":false,"type":"User"},"homeCollection":{"id":"b76595fb6cfc","name":"Open Machine Learning Course","slug":"open-machine-learning-course","tags":["MACHINE LEARNING","DATA SCIENCE","DATA ANALYSIS","EDUCATION","KAGGLE"],"creatorId":"5572ec600139","description":"A series of articles on basics of Machine Learning. Each article is followed by an assignment with a deadline. Several Kaggle Inclass competitions are held throughout the course.","shortDescription":"A series of articles on basics of Machine Learning.","image":{"imageId":"1*b5pfoCQz20rjh79OPSQWQg.jpeg","filter":"","backgroundSize":"","originalWidth":271,"originalHeight":271,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":1394,"activeAt":1523869737051},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false},"isSubscribed":true,"isNewsletterSubscribed":true,"memberOfMembershipPlanId":"","isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"1*MxsqShnPs3RXMScwl5HhsQ.jpeg","filter":"","backgroundSize":"","originalWidth":1479,"originalHeight":822,"strategy":"resample","height":0,"width":0},"twitterUsername":"odsai_en","facebookPageName":"DataChallenges","publicEmail":"yury.kashnitsky@gmail.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"Open Machine Learning Course","description":"by OpenDataScience","backgroundImage":{"id":"1*ZnqNzhbOlqb19tLgWDn8ww.png","originalWidth":2400,"originalHeight":1200},"logoImage":{},"alignment":2,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":6,"number":10,"postIds":[]}}],"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"Open Machine Learning Course","description":"by OpenDataScience","backgroundImage":{"id":"1*ZnqNzhbOlqb19tLgWDn8ww.png","originalWidth":2400,"originalHeight":1200},"logoImage":{},"alignment":2,"layout":6},"type":"Collection"},"homeCollectionId":"b76595fb6cfc","title":"Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors","detectedLanguage":"en","latestVersion":"2f4e00167733","latestPublishedVersion":"2f4e00167733","hasUnpublishedEdits":false,"latestRev":2210,"createdAt":1519020053240,"updatedAt":1525725827281,"acceptedAt":0,"firstPublishedAt":1519033525404,"latestPublishedAt":1525725825745,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!","bodyModel":{"paragraphs":[{"name":"7b0c","type":3,"text":"Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors","markups":[]},{"name":"83af","type":4,"text":"A fractal tree. Source","markups":[{"type":3,"start":16,"end":22,"href":"https://alvenka.deviantart.com/art/fractal-tree-34-366989431","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"1*5Tt3NDpNobfU_YeKglBRyQ.jpeg","originalWidth":1031,"originalHeight":775,"isFeatured":true}},{"name":"aea6","type":1,"text":"Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!","markups":[]},{"name":"c3b5","type":3,"text":"Article outline","markups":[]},{"name":"96e1","type":10,"text":"Introduction","markups":[]},{"name":"462e","type":10,"text":"Decision Tree","markups":[]},{"name":"2063","type":9,"text":"How to Build a Decision Tree","markups":[]},{"name":"0e94","type":9,"text":"Tree-building Algorithm","markups":[]},{"name":"740f","type":9,"text":"Other Quality Criteria for Splits in Classification Problems","markups":[]},{"name":"de8f","type":9,"text":"How a Decision Tree Works with Numerical Features","markups":[]},{"name":"9f91","type":9,"text":"Crucial Tree Parameters","markups":[]},{"name":"89a8","type":9,"text":"Class DecisionTreeClassifier in Scikit-learn","markups":[{"type":10,"start":6,"end":28}]},{"name":"22e0","type":9,"text":"Decision Tree in a Regression Problem","markups":[]},{"name":"1d31","type":1,"text":"3. Nearest Neighbors Method","markups":[]},{"name":"5c64","type":9,"text":"Nearest Neighbors Method in Real Applications","markups":[]},{"name":"3af0","type":9,"text":"Class KNeighborsClassifier in Scikit-learn","markups":[{"type":10,"start":6,"end":26}]},{"name":"6e0b","type":1,"text":"4. Choosing Model Parameters and Cross-Validation","markups":[]},{"name":"933e","type":1,"text":"5. Application Examples and Complex Cases","markups":[]},{"name":"c0d7","type":9,"text":"Decision trees and nearest neighbors method in a customer churn prediction task","markups":[]},{"name":"a127","type":9,"text":"Complex Case for Decision Trees","markups":[]},{"name":"e454","type":9,"text":"Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition","markups":[]},{"name":"5757","type":9,"text":"Complex Case for the Nearest Neighbors Method","markups":[]},{"name":"d290","type":1,"text":"6. Pros and Cons of Decision Trees and the Nearest Neighbors Method","markups":[]},{"name":"a033","type":1,"text":"7. Assignment #3","markups":[]},{"name":"617e","type":1,"text":"8. Useful resources","markups":[]},{"name":"11b6","type":1,"text":"The following material is better viewed as a Jupyter notebook and can be reproduced locally with Jupyter if you clone the course repository.","markups":[{"type":3,"start":45,"end":61,"href":"http://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb?flush_cache=true","title":"","rel":"","anchorType":0},{"type":3,"start":122,"end":139,"href":"https://github.com/Yorko/mlcourse_open","title":"","rel":"noopener nofollow","anchorType":0},{"type":1,"start":0,"end":140}]},{"name":"43ed","type":3,"text":"1. Introduction","markups":[]},{"name":"1041","type":1,"text":"Before we dive into the material for this week’s article, let’s talk about the kind of problem that we are going to solve and its place in the exciting field of machine learning. T. Mitchell’s book Machine Learning (1997) gives a classic, general definition of machine learning as follows:","markups":[{"type":2,"start":198,"end":214}]},{"name":"7873","type":6,"text":"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.","markups":[{"type":2,"start":52,"end":53},{"type":2,"start":90,"end":91},{"type":2,"start":116,"end":117},{"type":2,"start":150,"end":151},{"type":2,"start":168,"end":169},{"type":2,"start":196,"end":197}]},{"name":"2bfc","type":1,"text":"In the various problem settings T, P, and E can refer to completely different things. Some of the most popular tasks T in machine learning are the following:","markups":[{"type":1,"start":111,"end":138},{"type":2,"start":32,"end":33},{"type":2,"start":35,"end":36},{"type":2,"start":42,"end":43},{"type":2,"start":117,"end":118}]},{"name":"d5b9","type":9,"text":"classification of an instance to one of the categories based on its features;","markups":[]},{"name":"7c98","type":9,"text":"regression — prediction of a numerical target feature based on other features of an instance;","markups":[]},{"name":"e577","type":9,"text":"clustering — identifying partitions of instances based on the features of these instances so that the members within the groups are more similar to each other than those in the other groups;","markups":[]},{"name":"bb0b","type":9,"text":"anomaly detection — search for instances that are “greatly dissimilar” to the rest of the sample or to some group of instances;","markups":[]},{"name":"b3df","type":9,"text":"and so many more.","markups":[]},{"name":"ca89","type":1,"text":"A good overview is provided in the “Machine Learning basics” chapter of “Deep Learning” (by Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016).","markups":[{"type":3,"start":72,"end":87,"href":"http://www.deeplearningbook.org/","title":"","rel":"","anchorType":0}]},{"name":"c5c7","type":1,"text":"Experience E refers to data (we can’t go anywhere without it). Machine learning algorithms can be divided into those that are trained in supervised or unsupervised manner. In unsupervised learning tasks, one has a set consisting of instances described by a set of features. In supervised learning problems, there’s also a target variable, which is what we would like to be able to predict, known for each instance in a training set.","markups":[{"type":1,"start":0,"end":12},{"type":2,"start":11,"end":12},{"type":2,"start":137,"end":147},{"type":2,"start":151,"end":163},{"type":2,"start":214,"end":217},{"type":2,"start":232,"end":241},{"type":2,"start":264,"end":272},{"type":2,"start":322,"end":337},{"type":2,"start":419,"end":431}]},{"name":"9896","type":13,"text":"Example","markups":[]},{"name":"26c6","type":1,"text":"Classification and regression are supervised learning problems. For example, as a credit institution, we may want to predict loan defaults based on the data accumulated about our clients. Here, the experience E is the available training data: a set of instances (clients), a collection of features (such as age, salary, type of loan, past loan defaults, etc.) for each, and a target variable (whether they defaulted on the loan). This target variable is just a fact of loan default (1 or 0), so recall that this is a (binary) classification problem. If you were instead predicting by how much time the loan payment is overdue, this would become a regression problem.","markups":[{"type":2,"start":209,"end":210},{"type":2,"start":252,"end":261},{"type":2,"start":289,"end":297},{"type":2,"start":376,"end":391},{"type":2,"start":581,"end":597}]},{"name":"13bc","type":1,"text":"Finally, the third term used in the definition of machine learning is a metric of the algorithm’s performance evaluation P. Such metrics differ for various problems and algorithms, and we’ll discuss them as we study new algorithms. For now, we’ll refer to a simple metric for classification algorithms, the proportion of correct answers — accuracy — on the test set.","markups":[{"type":1,"start":72,"end":123},{"type":2,"start":121,"end":122},{"type":2,"start":339,"end":347}]},{"name":"91f4","type":1,"text":"Let’s take a look at two supervised learning problems: classification and regression.","markups":[]},{"name":"13a9","type":3,"text":"2. Decision Tree","markups":[]},{"name":"a33b","type":1,"text":"We begin our overview of classification and regression methods with one of the most popular ones — a decision tree. Decision trees are used in everyday life decisions, not just in machine learning. Flow diagrams are actually visual representations of decision trees. For example, Higher School of Economics publishes information diagrams to make the lives of its employees easier. Here is a snippet of instructions for publishing a paper on the Institution portal.","markups":[]},{"name":"e68d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*CIb1qGHEY-UptAWD.png","originalWidth":1498,"originalHeight":1108}},{"name":"132d","type":1,"text":"In terms of machine learning, one can see it as a simple classifier that determines the appropriate form of publication (book, article, chapter of the book, preprint, publication in the “Higher School of Economics and the Media”) based on the content (book, pamphlet, paper), type of journal, original publication type (scientific journal, proceedings), etc.","markups":[]},{"name":"446a","type":1,"text":"A decision tree is often a generalization of the experts’ experience, a means of sharing knowledge of a particular process. For example, before the introduction of scalable machine learning algorithms, the credit scoring task in the banking sector was solved by experts. The decision to grant a loan was made on the basis of some intuitively (or empirically) derived rules that could be represented as a decision tree.","markups":[]},{"name":"8d8e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*gaIruEZ7kHJeLz0bhUo_GQ.png","originalWidth":440,"originalHeight":298}},{"name":"9101","type":1,"text":"In our next case, we solve a binary classification problem (approve/deny a loan) on the grounds of “Age”, “Home-ownership”, “Income” and “Education”.","markups":[]},{"name":"d442","type":1,"text":"The decision tree as a machine learning algorithm is essentially the same thing as the diagram shown above; we incorporate a stream of logical rules of the form “feature a value is less than x and feature b value is less than y … =\x3e Category 1” into a tree-like data structure. The advantage of this algorithm is that they are easily interpretable. For example, using the above scheme, the bank can explain to the client why they were denied for a loan: e.g the client does not own a house and her income is less than 5,000.","markups":[{"type":2,"start":170,"end":171},{"type":2,"start":191,"end":193},{"type":2,"start":205,"end":206},{"type":2,"start":226,"end":227}]},{"name":"64da","type":1,"text":"As we’ll see later, many other models, although more accurate, do not have this property and can be regarded as more of a “black box” approach, where it is harder to interpret how the input data was transformed into the output. Due to this “understandability” and similarity to human decision-making (you can easily explain your model to your boss), decision trees have gained immense popularity. C4.5, a representative of this group of classification methods, is even the first in the list of 10 best data mining algorithms (“Top 10 Algorithms in Data Mining”, Knowledge and Information Systems, 2008. PDF).","markups":[{"type":3,"start":603,"end":606,"href":"http://www.cs.uvm.edu/%7Eicdm/algorithms/10Algorithms-08.pdf","title":"","rel":"","anchorType":0}]},{"name":"c675","type":3,"text":"How to Build a Decision Tree","markups":[]},{"name":"c9cb","type":1,"text":"Earlier, we saw that the decision to grant a loan is made based on age, assets, income, and other variables. But what variable to look at first? Let’s discuss a simple example where all the variables are binary.","markups":[]},{"name":"5536","type":1,"text":"Recall the game of “20 Questions”, which is often referenced when introducing decision trees. You’ve probably played this game — one person thinks of a celebrity while the other tries to guess by asking only “Yes” or “No” questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking “Is it Angelina Jolie?” would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking “Is the celebrity a woman?” would reduce the possibilities to roughly half. That is to say, the “gender” feature separates the celebrity dataset much better than other features like “Angelina Jolie”, “Spanish”, or “loves football.” This reasoning corresponds to the concept of information gain based on entropy.","markups":[]},{"name":"69be","type":13,"text":"Entropy","markups":[]},{"name":"195b","type":1,"text":"Shannon’s entropy is defined for a system with N possible states as follows:","markups":[]},{"name":"8325","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cf-lyJLXXEDSK2H7GN8ncw.png","originalWidth":353,"originalHeight":133}},{"name":"afc5","type":1,"text":"where Pi is the probability of finding the system in the i-th state. This is a very important concept used in physics, information theory, and other areas. Entropy can be described as the degree of chaos in the system. The higher the entropy, the less ordered the system and vice versa. This will help us formalize “effective data splitting”, which we alluded to in the context of “20 Questions”.","markups":[{"type":2,"start":6,"end":9},{"type":2,"start":57,"end":58}]},{"name":"9599","type":13,"text":"Toy Example","markups":[]},{"name":"597c","type":1,"text":"To illustrate how entropy can help us identify good features for building a decision tree, let’s look at a toy example. We will predict the color of the ball based on its position.","markups":[]},{"name":"90e9","type":4,"text":"Source (in Russian)","markups":[{"type":3,"start":0,"end":6,"href":"https://habrahabr.ru/post/171759/","title":"","rel":"","anchorType":0}],"layout":1,"metadata":{"id":"0*jW6I6V_I-Nn725It.png","originalWidth":606,"originalHeight":96}},{"name":"2033","type":1,"text":"There are 9 blue balls and 11 yellow balls. If we randomly pull out a ball, then it will be blue with probability p1 = 9/20 and yellow with probability p2 = 11/20, which gives us an entropy S0 = -9/20 log2(9/20) - 11/20 log2(11/20) ≈ 1. This value by itself may not tell us much, but let’s see how the value changes if we were to break the balls into two groups: with the position less than or equal to 12 and greater than 12.","markups":[]},{"name":"72b9","type":4,"text":"Source (in Russian)","markups":[{"type":3,"start":0,"end":6,"href":"https://habrahabr.ru/post/171759/","title":"","rel":"noopener","anchorType":0}],"layout":1,"metadata":{"id":"0*JSFOJNkdOHNhSjdo.png","originalWidth":717,"originalHeight":220}},{"name":"a28a","type":1,"text":"The left group has 13 balls, 8 blue and 5 yellow. The entropy of this group is S1 = -5/13 log2(5/13) - 8/13 log2(8/13) ≈ 0.96. The right group has 7 balls, 1 blue and 6 yellow. The entropy of the right group is S2 = -1/7 log2(1/7) - 6/7 log2(6/7) ≈ 0.6. As you can see, entropy has decreased in both groups, more so in the right group. Since entropy is, in fact, the degree of chaos (or uncertainty) in the system, the reduction in entropy is called information gain. Formally, the information gain (IG) for a split based on the variable Q(in this example it’s a variable “x ≤ 12”) is defined as","markups":[{"type":2,"start":538,"end":539},{"type":2,"start":573,"end":579}]},{"name":"023b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ewOFVFQEXRwamSPBwY2QGw.png","originalWidth":429,"originalHeight":132}},{"name":"0b84","type":1,"text":"where q is the number of groups after the split, Ni is number of objects from the sample in which variable Q is equal to the i-th value. In our example, our split yielded two groups (q = 2), one with 13 elements (N1 = 13), the other with 7 (N2 = 7). Therefore, we can compute the information gain as","markups":[{"type":2,"start":6,"end":8},{"type":2,"start":49,"end":51},{"type":2,"start":107,"end":108},{"type":2,"start":125,"end":126}]},{"name":"ca5d","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*wNF6UPLh_H-J0u70tlQU1g.png","originalWidth":703,"originalHeight":97}},{"name":"12b7","type":1,"text":"It turns out that dividing the balls into two groups by splitting on “coordinate is less than or equal to 12” gave us a more ordered system. Let’s continue to divide them into groups until the balls in each group are all of the same color.","markups":[]},{"name":"2816","type":1,"text":"For the right group, we can easily see that we only need one extra partition using “coordinate less than or equal to 18”. But, for the left group, we need three more. Note that the entropy of a group where all of the balls are the same color is equal to 0 (log2(1) = 0).","markups":[]},{"name":"5466","type":1,"text":"We have successfully constructed a decision tree that predicts ball color based on its position. This decision tree may not work well if we add any balls because it has perfectly fit to the training set (initial 20 balls). If we wanted to do well in that case, a tree with fewer “questions” or splits would be more accurate, even if it does not perfectly fit the training set. We will discuss the problem of overfitting later.","markups":[]},{"name":"07cc","type":3,"text":"Tree-building Algorithm","markups":[]},{"name":"028f","type":1,"text":"We can make sure that the tree built in the previous example is optimal: it took only 5 “questions” (conditioned on the variable x) to perfectly fit a decision tree to the training set. Under other split conditions, the resulting tree would be deeper, i.e. take more “questions” to reach an answer.","markups":[{"type":2,"start":129,"end":130}]},{"name":"75c6","type":1,"text":"At the heart of the popular algorithms for decision tree construction, such as ID3 or C4.5, lies the principle of greedy maximization of information gain: at each step, the algorithm chooses the variable that gives the greatest information gain upon splitting. Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting). Different algorithms use different heuristics for “early stopping” or “cut-off” to avoid constructing an overfitted tree.","markups":[]},{"name":"56e4","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*cKuUJ3ARs8tTk15Of_4btg.png","originalWidth":718,"originalHeight":276}},{"name":"1ac6","type":3,"text":"Other Quality Criteria for Splits in Classification Problems","markups":[]},{"name":"34df","type":1,"text":"We discussed how entropy allows us to formalize partitions in a tree. But this is only one heuristic; there exist others.","markups":[]},{"name":"1986","type":4,"text":"Gini uncertainty (Gini impurity)","markups":[],"layout":1,"metadata":{"id":"1*tQO9YrY-ntGAjM_RzW-pkA.png","originalWidth":297,"originalHeight":106}},{"name":"1efd","type":1,"text":"Maximizing this criterion can be interpreted as the maximization of the number of pairs of objects of the same class that are in the same subtree (not to be confused with the Gini index).","markups":[]},{"name":"b1e5","type":4,"text":"Misclassification error","markups":[],"layout":1,"metadata":{"id":"1*WOB8ZXlkfv-DLHvzaZl88A.png","originalWidth":271,"originalHeight":72}},{"name":"6d89","type":1,"text":"In practice, misclassification error is almost never used, and Gini uncertainty and information gain work similarly.","markups":[]},{"name":"5e8c","type":1,"text":"For binary classification, entropy and Gini uncertainty take the following form:","markups":[]},{"name":"004b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*ijHoiJ08hV0KckUbIMERcw.png","originalWidth":738,"originalHeight":84}},{"name":"ba46","type":1,"text":"where (p+ is the probability of an object having a label +).","markups":[{"type":2,"start":7,"end":9}]},{"name":"328b","type":1,"text":"If we plot these two functions against the argument p+, we will see that the entropy plot is very close to the plot of Gini uncertainty, doubled. Therefore, in practice, these two criteria are almost identical.","markups":[{"type":2,"start":52,"end":54}]},{"name":"17d9","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*GC23qi3fl2hrWW4eUComgw.png","originalWidth":1800,"originalHeight":1200}},{"name":"72af","type":13,"text":"Example","markups":[]},{"name":"41c9","type":1,"text":"Let’s consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means.","markups":[]},{"name":"77b3","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"2ef293179efea50d277703e91e4768df","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"c6b9","type":1,"text":"Let’s plot the data. Informally, the classification problem in this case is to build some “good” boundary separating the two classes (the red dots from the yellow). Machine learning for this case boils down to choosing a good separating border. A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new samples. Intuitively, some smooth boundary, or, at least, a straight line or a hyperplane, would work well on new data.","markups":[]},{"name":"627a","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"fc55ac1553e94cc62ff5b94fc3b0b6c2","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"989e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*77R2NOZymudraI89GH9LuA.png","originalWidth":3000,"originalHeight":2400}},{"name":"6d46","type":1,"text":"Let’s try to separate these two classes by training an Sklearn decision tree. We will use max_depth parameter that limits the depth of the tree. Let's visualize the resulting separating boundary.","markups":[{"type":10,"start":55,"end":62},{"type":10,"start":90,"end":99}]},{"name":"5ab5","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"f274a20bcb6b98c87a68863e5dd4c043","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"ed2b","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*4t4IrcQdKplQsAE4SBduXg.png","originalWidth":3000,"originalHeight":2400}},{"name":"6703","type":1,"text":"And how does the tree itself look? We see that the tree “cuts” the space into 8 rectangles, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it.","markups":[]},{"name":"39ff","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"664fe19fd7f66fcee668fa5042fea92f","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"d6f7","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*aNhu3p8AkkGs3uJlKdLUjg.png","originalWidth":1227,"originalHeight":477}},{"name":"c0fc","type":13,"text":"How can we “read” such a tree?","markups":[]},{"name":"e23e","type":1,"text":"In the beginning, there were 200 samples (instances), 100 of each class. The entropy of the initial state was maximal, S=1. Then, the first partition of the samples into 2 groups was made by comparing the value of x2 with 1.211 (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white.","markups":[{"type":2,"start":214,"end":216}]},{"name":"5262","type":3,"text":"How a Decision Tree Works with Numerical Features","markups":[]},{"name":"15c7","type":1,"text":"Suppose we have a numeric feature “Age” that has a lot of unique values. A decision tree will look for the best (according to some criterion of information gain) split by checking binary attributes such as “Age \x3c17”, “Age \x3c 22.87”, and so on. But what if the age range is large? Or what if another quantitative variable, “salary”, can also be “cut” in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.","markups":[]},{"name":"2c79","type":1,"text":"Let’s consider an example. Suppose we have the following dataset:","markups":[]},{"name":"bdc4","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"dfb2658bcc651357a93115d2ac6bf5ea","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"27a8","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*d17quX4zhAmYEe_JJ_IN4g.png","originalWidth":220,"originalHeight":489}},{"name":"a2de","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"d9600bc0df919950bed4057878bb0409","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"480c","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*a03Xr_5-wfKDzWoqHUiyyg.png","originalWidth":499,"originalHeight":744}},{"name":"ec7e","type":1,"text":"We see that the tree used the following 5 values to evaluate by age: 43.5, 19, 22.5, 30, and 32 years. If you look closely, these are exactly the mean values between the ages at which the target class “switches” from 1 to 0 or 0 to 1. To illustrate further, 43.5 is the average of 38 and 49 years; a 38-year-old customer failed to return the loan whereas the 49-year-old did. The tree looks for the values at which the target class switches its value as a threshold for “cutting” a quantitative variable.","markups":[]},{"name":"9476","type":1,"text":"Given this information, why do you think it makes no sense here to consider a feature like “Age \x3c17.5”?","markups":[]},{"name":"8e13","type":1,"text":"Let’s consider a more complex example by adding the “Salary” variable (in the thousands of dollars per year).","markups":[]},{"name":"af52","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"abc0df2a49847ff6bd541717eee0ba0d","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"d782","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*6VJBLqoPTy07sJLQa3iNrQ.png","originalWidth":292,"originalHeight":494}},{"name":"10ce","type":1,"text":"If we sort by age, the target class ( “loan default”) switches (from 1 to 0 or vice versa) 5 times. And if we sort by salary, it switches 7 times. How will the tree choose features now? Let’s see.","markups":[]},{"name":"1826","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"ebf0b04c2d2276e599cedb424e119acd","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"3a74","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*pZbJ-J1Q_UW-AH1KrhbiBA.png","originalWidth":505,"originalHeight":611}},{"name":"adb3","type":1,"text":"We see that the tree partitioned by both salary and age. Moreover, the thresholds for feature comparisons are 43.5 and 22.5 years of age and 95k and 30.5k per year. Again, we see that 95 is the average between 88 and 102; the individual with a salary of 88k proved to be “bad” while the one with 102k was “good”. The same goes for 30.5k. That is, only a few values for comparisons by age and salary were searched. Why did the tree choose these features? Because they gave better partitioning (according to Gini uncertainty).","markups":[]},{"name":"02d7","type":1,"text":"Conclusion: the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes.","markups":[{"type":1,"start":0,"end":10}]},{"name":"a3b2","type":1,"text":"Furthermore, when there are a lot of numeric features in a dataset, each with a lot of unique values, only the top-N of the thresholds described above are selected, i.e. only use the top-N that give maximum gain. The process is to construct a tree of depth 1, compute the entropy (or Gini uncertainty), and select the best thresholds for comparison.","markups":[]},{"name":"dc71","type":1,"text":"To illustrate, if we split by “Salary ≤ 34.5”, the left subgroup will have the entropy of 0 (all clients are “bad”), and the right one will have the entropy of 0.954 (3 “bad” and 5 “good”, you can check this yourself as it will be part of the assignment). The information gain is roughly 0.3. If we split by “Salary ≤ 95”, the left subgroup will have an entropy of 0.97 (6 “bad” and 4 “good”), and the right one will have an entropy of 0 (a group containing only one object). The information gain is about 0.11. If we calculate information gain for each partition in that manner, we can select the thresholds for comparison of each numeric feature before the construction of a large tree (using all features).","markups":[]},{"name":"8250","type":1,"text":"More examples of numeric feature discretization can be found in posts like this or this. One of the most prominent scientific papers on this subject is “On the handling of continuous-valued attributes in decision tree generation” (UM Fayyad. KB Irani, “Machine Learning”, 1992).","markups":[{"type":3,"start":75,"end":79,"href":"http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/","title":"","rel":"","anchorType":0},{"type":3,"start":83,"end":87,"href":"http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx","title":"","rel":"","anchorType":0}]},{"name":"8b95","type":3,"text":"Crucial Tree Parameters","markups":[]},{"name":"dcee","type":1,"text":"Technically, you can build a decision tree until each leaf has exactly one instance, but this is not common in practice when building a single tree because it will be overfitted, or too tuned to the training set, and will not predict labels for new data well. At the bottom of the tree, at some great depth, there will be partitions on less important features (e.g. whether a client came from Leeds or New York). We can exaggerate this story further and find that all four clients who came to the bank for a loan in green trousers did not return the loan. Even if that were true in training, we do not want our classification model to generate such specific rules.","markups":[{"type":2,"start":167,"end":177}]},{"name":"6dcf","type":1,"text":"There are two exceptions where the trees are built to the maximum depth:","markups":[]},{"name":"6578","type":9,"text":"Random Forest (a group of trees) averages the responses from individual trees that are built to the maximum depth (we will talk later on why you should do this)","markups":[]},{"name":"e13c","type":9,"text":"Pruning trees. In this approach, the tree is first constructed to the maximum depth. Then, from the bottom up, some nodes of the tree are removed by comparing the quality of the tree with and without that partition (comparison is performed using cross-validation, more on this below).","markups":[{"type":2,"start":0,"end":7},{"type":2,"start":246,"end":262}]},{"name":"f68e","type":1,"text":"The picture below is an example of a dividing border built in an overfitted tree.","markups":[]},{"name":"5bc7","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*Lg34OEoe_VlX2-1n.png","originalWidth":590,"originalHeight":465}},{"name":"d9f9","type":1,"text":"The most common ways to deal with overfitting in decision trees are as follows:","markups":[]},{"name":"6054","type":9,"text":"artificial limitation of the depth or a minimum number of samples in the leaves: the construction of a tree just stops at some point;","markups":[]},{"name":"8ce7","type":9,"text":"pruning the tree.","markups":[]},{"name":"9b72","type":3,"text":"Class DecisionTreeClassifier in Scikit-learn","markups":[]},{"name":"d1c7","type":1,"text":"The main parameters of the sklearn.tree.DecisionTreeClassifier class are:","markups":[{"type":10,"start":27,"end":62},{"type":3,"start":27,"end":62,"href":"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html","title":"","rel":"","anchorType":0}]},{"name":"81ce","type":9,"text":"max_depth – the maximum depth of the tree;","markups":[{"type":10,"start":0,"end":9}]},{"name":"d31e","type":9,"text":"max_features – the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be \"expensive\" to search for partitions for all features);","markups":[{"type":10,"start":0,"end":12},{"type":2,"start":202,"end":205}]},{"name":"7227","type":9,"text":"min_samples_leaf – the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.","markups":[{"type":10,"start":0,"end":16}]},{"name":"8874","type":1,"text":"The parameters of the tree need to be set depending on input data, and it is usually done by means of cross-validation, more on this below.","markups":[{"type":2,"start":102,"end":118}]},{"name":"4b9f","type":3,"text":"Decision Tree in a Regression Problem","markups":[]},{"name":"0f08","type":1,"text":"When predicting a numeric variable, the idea of a tree construction remains the same, but the quality criteria changes.","markups":[]},{"name":"af04","type":4,"text":"Variance around the mean","markups":[],"layout":1,"metadata":{"id":"1*iiqWU5CR4F1F_8aQFaOpvg.png","originalWidth":455,"originalHeight":135}},{"name":"4e3a","type":1,"text":"where n is the number of samples in a leaf, Yi is the value of the target variable. Simply put, by minimizing the variance around the mean, we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.","markups":[{"type":2,"start":6,"end":7},{"type":2,"start":44,"end":47}]},{"name":"9017","type":13,"text":"Example","markups":[]},{"name":"90a5","type":1,"text":"Let’s generate some data distributed by the function","markups":[]},{"name":"dd75","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*JlU7gNyjvAJDLc9hbLJVKg.png","originalWidth":257,"originalHeight":43}},{"name":"3847","type":1,"text":"with some noise. Then we will train a tree on it and show what predictions it makes.","markups":[]},{"name":"4c42","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"fa9377033bbe22e1bcd9e2f6439d8b5a","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"7012","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*-_dzWXtxS6XpmZemkKacnw.png","originalWidth":604,"originalHeight":370}},{"name":"e255","type":1,"text":"We see that the decision tree approximates the data with a piecewise constant function.","markups":[]},{"name":"267c","type":3,"text":"3. Nearest Neighbors Method","markups":[]},{"name":"2627","type":1,"text":"The nearest neighbors method (k-Nearest Neighbors, or k-NN) is another very popular classification method that is also sometimes used in regression problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class.","markups":[{"type":2,"start":0,"end":28}]},{"name":"c305","type":1,"text":"According to the nearest neighbors method, the green ball would be classified as “blue” rather than “red”.","markups":[]},{"name":"e830","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"0*TMLaS4woI6xo6RKs.png","originalWidth":404,"originalHeight":262}},{"name":"8a93","type":1,"text":"For another example, if you do not know how to tag a Bluetooth-headset on an online listing, you can find 5 similar headsets, and, if 4 of them are tagged as “accessories” and only 1 as “Technology”, then you will also label it under “accessories”.","markups":[]},{"name":"1be0","type":1,"text":"To classify each sample from the test set, one needs to perform the following operations in order:","markups":[]},{"name":"5b71","type":10,"text":"Calculate the distance to each of the samples in the training set.","markups":[]},{"name":"109c","type":10,"text":"Select k samples from the training set with the minimal distance to them.","markups":[{"type":2,"start":7,"end":8}]},{"name":"28e4","type":10,"text":"The class of the test sample will be the most frequent class among those k nearest neighbors.","markups":[{"type":2,"start":73,"end":74}]},{"name":"491c","type":1,"text":"The method adapts quite easily for the regression problem: on step 3, it returns not the class, but the number — a mean (or median) of the target variable among neighbors.","markups":[]},{"name":"4721","type":1,"text":"A notable feature of this approach is its laziness — calculations are only done during the prediction phase, when a test sample needs to be classified. No model is constructed from the training examples beforehand. In contrast, recall that for decision trees in the first half of this article the tree is constructed based on the training set, and the classification of test cases occurs relatively quickly by traversing through the tree.","markups":[]},{"name":"3421","type":1,"text":"Nearest neighbors is a well-studied approach. There exist many important theorems claiming that, on “endless” datasets, it is the optimal method of classification. The authors of the classic book “The Elements of Statistical Learning” consider k-NN to be a theoretically ideal algorithm which usage is only limited by computation power and the curse of dimensionality.","markups":[{"type":3,"start":344,"end":367,"href":"https://en.wikipedia.org/wiki/Curse_of_dimensionality","title":"","rel":"","anchorType":0}]},{"name":"c787","type":3,"text":"Nearest Neighbors Method in Real Applications","markups":[]},{"name":"2456","type":9,"text":"k-NN can serve as a good starting point (baseline) in some cases;","markups":[]},{"name":"a82c","type":9,"text":"In Kaggle competitions, k-NN is often used for the construction of meta-features (i.e. k-NN predictions as input to other models) or for stacking/blending;","markups":[]},{"name":"c7cf","type":9,"text":"The nearest neighbors method extends to other tasks like recommendation systems. The initial decision could be a recommendation of a product (or service) that is popular among the closest neighbors of the person for whom we want to make a recommendation;","markups":[{"type":2,"start":180,"end":197}]},{"name":"6bac","type":9,"text":"In practice, on large datasets, approximate methods of search are often used for nearest neighbors. There is a number of open source libraries that implement such algorithms; check out Spotify’s library Annoy.","markups":[{"type":3,"start":203,"end":208,"href":"https://github.com/spotify/annoy","title":"","rel":"","anchorType":0}]},{"name":"3095","type":1,"text":"The quality of classification/regression with k-NN depends on several parameters:","markups":[]},{"name":"b520","type":9,"text":"The number of neighbors k.","markups":[{"type":2,"start":24,"end":25}]},{"name":"d3b6","type":9,"text":"The distance measure between samples (common ones include Hamming, Euclidean, cosine, and Minkowski distances). Note that most of these metrics require data to be scaled. Simply speaking, we do not want the “salary” feature, which is on the order of thousands, to affect the distance more than “age”, which is generally less than 100.","markups":[]},{"name":"301d","type":9,"text":"Weights of neighbors (each neighbor may contribute different weights; for example, the further the sample, the lower the weight).","markups":[]},{"name":"12e4","type":3,"text":"Class KNeighborsClassifier in Scikit-learn","markups":[{"type":10,"start":6,"end":26}]},{"name":"960e","type":1,"text":"The main parameters of the class sklearn.neighbors.KNeighborsClassifier are:","markups":[{"type":10,"start":33,"end":71}]},{"name":"122b","type":9,"text":"weights: uniform (all weights are equal), distance (the weight is inversely proportional to the distance from the test sample), or any other user-defined function;","markups":[{"type":10,"start":9,"end":16},{"type":10,"start":42,"end":50}]},{"name":"4d06","type":9,"text":"algorithm (optional): brute, ball_tree, KD_tree, or auto. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to auto, the right way to find the neighbors will be automatically chosen based on the training set.","markups":[{"type":10,"start":22,"end":27},{"type":10,"start":29,"end":38},{"type":10,"start":40,"end":47},{"type":10,"start":52,"end":56},{"type":10,"start":328,"end":332}]},{"name":"bd2e","type":9,"text":"leaf_size (optional): threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;","markups":[]},{"name":"8886","type":9,"text":"metric: minkowski, manhattan, euclidean, chebyshev, or other.","markups":[{"type":10,"start":8,"end":17},{"type":10,"start":19,"end":28},{"type":10,"start":30,"end":39},{"type":10,"start":41,"end":50}]},{"name":"1405","type":3,"text":"4. Choosing Model Parameters and Cross-Validation","markups":[]},{"name":"de4e","type":1,"text":"The main task of learning algorithms is to be able to generalize to unseen data. Since we cannot immediately check the model performance on new, incoming data (because we do not know the true values of the target variable yet), it is necessary to sacrifice a small portion of the data to check the quality of the model on it.","markups":[{"type":2,"start":54,"end":64}]},{"name":"b81a","type":1,"text":"This is often done in one of two ways:","markups":[]},{"name":"fce6","type":9,"text":"setting aside a part of the dataset (held-out/hold-out set). We reserve a fraction of the training set (typically from 20% to 40%), train the model on the remaining data (60–80% of the original set), and compute performance metrics for the model (e.g accuracy) on the hold-out set.","markups":[{"type":2,"start":37,"end":58}]},{"name":"af5d","type":9,"text":"cross-validation. The most frequent case here is k-fold cross-validation.","markups":[{"type":2,"start":0,"end":16},{"type":2,"start":49,"end":72}]},{"name":"a1cf","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"0*iGVu2OmIL1WUSjZF.png","originalWidth":1000,"originalHeight":302}},{"name":"cd64","type":1,"text":"In k-fold cross-validation, the model is trained K times on different (K-1) subsets of the original dataset (in white) and checked on the remaining subset (each time a different one, shown above in orange). We obtain K model quality assessments that are usually averaged to give an overall average quality of classification/regression.","markups":[{"type":2,"start":49,"end":50},{"type":2,"start":71,"end":74},{"type":2,"start":217,"end":218}]},{"name":"3590","type":1,"text":"Cross-validation provides a better assessment of the model quality on new data compared to the hold-out set approach. However, cross-validation is computationally expensive when you have a lot of data.","markups":[]},{"name":"04ad","type":1,"text":"Cross-validation is a very important technique in machine learning and can also be applied in statistics and econometrics. It helps with hyperparameter tuning, model comparison, feature evaluation, etc. More details can be found here (blog post by Sebastian Raschka) or in any classic textbook on machine (statistical) learning.","markups":[{"type":3,"start":229,"end":233,"href":"https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html","title":"","rel":"","anchorType":0}]},{"name":"1229","type":3,"text":"5. Application Examples and Complex Cases","markups":[]},{"name":"d662","type":3,"text":"Decision trees and nearest neighbors method in a customer churn prediction task","markups":[]},{"name":"be35","type":1,"text":"Let’s read data into a DataFrame and preprocess it. Store State in a separate Series object for now and remove it from the dataframe. We will train the first model without the State feature, and then we will see if it helps.","markups":[{"type":10,"start":23,"end":32},{"type":10,"start":78,"end":84},{"type":2,"start":58,"end":63},{"type":2,"start":176,"end":181}]},{"name":"6034","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"d2df271a0611166325e3f2d13e7da48e","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"d848","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_ayZZheYD8jFveMgXZ_7fA.png","originalWidth":988,"originalHeight":188}},{"name":"4ed7","type":1,"text":"Let’s allocate 70% of the set for training (X_train, y_train) and 30% for the hold-out set (X_holdout, y_holdout). The hold-out set will not be involved in tuning the parameters of the models. We'll use it at the end, after tuning, to assess the quality of the resulting model. Let's train 2 models: a decision tree and k-NN. We do not know what parameters are good, so we will assume some random ones: a tree depth of 5 and the number of nearest neighbors equal 10.","markups":[{"type":10,"start":44,"end":51},{"type":10,"start":53,"end":60},{"type":10,"start":92,"end":101},{"type":10,"start":103,"end":112}]},{"name":"ef2a","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"c50323ecd9e9d8ead17911341cb3b122","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"6cec","type":1,"text":"Let’s assess prediction quality on our hold-out set with a simple metric — the proportion of correct answers (accuracy). The decision tree did better — percentage of correct answers is about 94% (decision tree) versus 88% (k-NN). Note that this performance is achieved by using random parameters.","markups":[]},{"name":"fe75","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"12e78c34ff52832de27915dfe278b686","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"9a5a","type":1,"text":"Now, let’s identify the parameters for the tree using cross-validation. We’ll tune the maximum depth and the maximum number of features used at each split. Here is the essence of how the GridSearchCV works: for each unique pair of values of max_depth and max_features, compute model performance with 5-fold cross-validation, and then select the best combination of parameters.","markups":[{"type":10,"start":241,"end":250},{"type":10,"start":255,"end":267}]},{"name":"6008","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"f203d8b929e433a538f1e44a0447e2ae","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"2d54","type":1,"text":"Let’s list the best parameters and the corresponding mean accuracy from cross-validation.","markups":[]},{"name":"260b","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"4d88a39f1ed85538a4b253c7fba32e6f","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"f956","type":1,"text":"Let’s draw the resulting tree. Due to the fact that it is not entirely a toy example (its maximum depth is 6), the picture is not that small, but you can “walk” over the tree if you locally open the corresponding picture downloaded from the course repo.","markups":[]},{"name":"223a","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"7b9b552e1176b50bc0bc267d9a5cdd79","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"440b","type":4,"text":"","markups":[],"layout":5,"metadata":{"id":"1*yBZp1DXgtxBHsi8UxeC6Cg.png","originalWidth":4431,"originalHeight":877}},{"name":"e488","type":1,"text":"Now, let’s tune the number of neighbors k for k-NN:","markups":[{"type":2,"start":40,"end":41}]},{"name":"0c2e","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"b44bdf193962dfd5166d8014e95e895d","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"46e9","type":1,"text":"Here, the tree proved to be better than the nearest neighbors algorithm: 94.2%/96.6% accuracy for cross-validation and hold-out respectively. Decision trees perform very well, and even random forest (let’s think of it for now as a bunch of trees that work better together) in this example cannot achieve better performance (95.1%/95.3%) despite being trained for much longer.","markups":[]},{"name":"ae24","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"8623f3bc35337b021f651c2b6f380456","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"c89e","type":3,"text":"Complex Case for Decision Trees","markups":[]},{"name":"8192","type":1,"text":"To continue the discussion of the pros and cons of the methods in question, let’s consider a simple classification task, where a tree would perform well but does it in an “overly complicated” manner. Let’s create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line.","markups":[]},{"name":"6e78","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"eb61c66d2367a4a7732b668abfce8ca7","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"8c38","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*oKfEmX5DBMev8e_Hqnv53g.png","originalWidth":590,"originalHeight":465}},{"name":"2af6","type":1,"text":"However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the 30 x 30 squares that frame the training set.","markups":[]},{"name":"ede6","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"7ae048fab32bcc36d0de556428ba04e1","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"d42e","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_OqTKeKHCUx0J38T7gGTYw.png","originalWidth":3000,"originalHeight":2400}},{"name":"203b","type":1,"text":"We got this overly complex construction, although the solution is just a straight line x1 = x2.","markups":[{"type":2,"start":87,"end":94}]},{"name":"cf60","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"1af7e72326579eeeec067b07233bc5d6","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"da44","type":4,"text":"","markups":[],"layout":3,"metadata":{"id":"1*oDkLhpY3kfa87UsjIp51ZA.png","originalWidth":2583,"originalHeight":744}},{"name":"acc3","type":1,"text":"The method of one nearest neighbor does better than the tree but is still not as good as a linear classifier (our next topic).","markups":[{"type":3,"start":114,"end":124,"href":"https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220","title":"","rel":"","anchorType":0}]},{"name":"5cc0","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"6b4d3434376c2d263163367449a149e3","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"9729","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*CSUaToczckWZGPf7wsKAKA.png","originalWidth":596,"originalHeight":479}},{"name":"d97c","type":3,"text":"Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition","markups":[]},{"name":"ae33","type":1,"text":"Now let’s have a look at how these 2 algorithms perform on a real-world task. We will use the sklearn built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.","markups":[{"type":10,"start":94,"end":101}]},{"name":"bb4e","type":1,"text":"Pictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is ​​”unfolded” into a vector of length 64, and we obtain a feature description of an object.","markups":[]},{"name":"e552","type":1,"text":"Let’s draw some handwritten digits. We see that they are distinguishable.","markups":[]},{"name":"b1ec","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"eeb9bf710715c6ef43864db717033080","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"37f0","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*u8zadz3PKNo3fOWFJm_vDA.png","originalWidth":919,"originalHeight":356}},{"name":"f8e0","type":1,"text":"Next, let’s do the same experiment as in the previous task, but, this time, let’s change the ranges for tunable parameters.","markups":[]},{"name":"e599","type":1,"text":"Let’s select 70% of the dataset for training (X_train, y_train) and 30% for holdout (X_holdout, y_holdout). The holdout set will not participate in model parameters tuning; we will use it at the end to check the quality of the resulting model.","markups":[{"type":10,"start":46,"end":53},{"type":10,"start":55,"end":62},{"type":10,"start":85,"end":94},{"type":10,"start":96,"end":105}]},{"name":"bd9a","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"d18f2fa52ae89eb4182722b57dde7a19","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"a477","type":1,"text":"Let’s train a decision tree and k-NN with our random parameters and make predictions on our holdout set. We can see that k-NN did much better, but note that this is with random parameters.","markups":[]},{"name":"571c","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"af1edbb0c7f4af8a297f4b289f55d815","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"8b35","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"2aed2efa0ee6555018d2ccecd4f3c680","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"c2fb","type":1,"text":"Now let’s tune our model parameters using cross-validation as before, but now we’ll take into account that we have more features than in the previous task: 64.","markups":[]},{"name":"39b7","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"85b385de90910523b1bb8c1969db5bed","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"e12e","type":1,"text":"Let’s see the best parameters combination and the corresponding accuracy from cross-validation:","markups":[]},{"name":"587b","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"36b64aa1c99ee8d214c83773a7964bff","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"86a2","type":1,"text":"That has already passed 66% but not quite 97%. k-NN works better on this dataset. In the case of one nearest neighbor, we were able to reach 99% guesses on cross-validation.","markups":[]},{"name":"9d9a","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"9bb1f93487ab77f352d87d17165392ee","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"0315","type":1,"text":"Let’s train a random forest on the same dataset, it works better than k-NN on the majority of datasets. But we here have an exception.","markups":[]},{"name":"64b7","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"66ac4a6910d9a2ec12ce8721a974fb91","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"4bec","type":1,"text":"You would be right to point out that we have not tuned any RandomForestClassifier parameters here. Even with tuning, the training accuracy doesn’t reach 98% as it did with one nearest neighbor.","markups":[{"type":10,"start":59,"end":81}]},{"name":"26d4","type":4,"text":"CV and Holdout are average shares of the correct answers on cross-model validation and hold-out sample. DT stands for a decision tree, k-NN stands for k-nearest neighbors, RF stands for random forest","markups":[{"type":2,"start":0,"end":199}],"layout":1,"metadata":{"id":"1*9ZRg1FTetV7cMI_oNNcD_w.png","originalWidth":288,"originalHeight":205}},{"name":"4fb7","type":1,"text":"The conclusion of this experiment (and general advice): first check simple models on your data: decision tree and nearest neighbors (next time we will also add logistic regression to this list). It might be the case that these methods already work well enough.","markups":[{"type":1,"start":4,"end":14}]},{"name":"4634","type":3,"text":"Complex Case for the Nearest Neighbors Method","markups":[]},{"name":"7c03","type":1,"text":"Let’s consider another simple example. In the classification problem, one of the features will just be proportional to the vector of responses, but this won’t help for the nearest neighbors method.","markups":[]},{"name":"e406","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"559da4052b71b58187c316d9173cfde1","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"e9ff","type":1,"text":"As always, we will look at the accuracy for cross-validation and the hold-out set. Let’s construct curves reflecting the dependence of these quantities on the n_neighbors parameter in the method of nearest neighbors. These curves are called validation curves.","markups":[{"type":10,"start":159,"end":170}]},{"name":"88dc","type":1,"text":"One can see that k-NN with the Euclidean distance does not work well on the problem, even when you vary the number of nearest neighbors over a wide range.","markups":[]},{"name":"83ee","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"52ee260c895c98755c6cb3ef99d3a6ce","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"0b9c","type":4,"text":"","markups":[],"layout":1,"metadata":{"id":"1*_jt-Vi1_0IRsC0riZ1nb6g.png","originalWidth":598,"originalHeight":479}},{"name":"048e","type":1,"text":"In contrast, the decision tree easily “detects” hidden dependencies in the data despite a restriction on the maximum depth.","markups":[]},{"name":"1b54","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"6b09e4c1be2d25a3ad3ebcdabdbb502f","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars1.githubusercontent.com%2Fu%2F3973673%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"e3be","type":1,"text":"In the second example, the tree solved the problem perfectly while k-NN experienced difficulties. However, this is more of a disadvantage of using Euclidian distance than of the method. It did not allow us to reveal that one feature was much better than the others.","markups":[]},{"name":"955e","type":3,"text":"6. Pros and Cons of Decision Trees and the Nearest Neighbors Method","markups":[]},{"name":"95f1","type":13,"text":"Pros and cons of decision trees","markups":[]},{"name":"c057","type":1,"text":"Pros:","markups":[]},{"name":"3985","type":9,"text":"Generation of clear human-understandable classification rules, e.g. “if age \x3c25 and is interested in motorcycles, deny the loan”. This property is called interpretability of the model.","markups":[]},{"name":"e998","type":9,"text":"Decision trees can be easily visualized, i.e. both the model itself (the tree) and prediction for a certain test object (a path in the tree) can “be interpreted”.","markups":[]},{"name":"4d6e","type":9,"text":"Fast training and forecasting.","markups":[]},{"name":"7563","type":9,"text":"Small number of model parameters.","markups":[]},{"name":"3b73","type":9,"text":"Supports both numerical and categorical features.","markups":[]},{"name":"6aad","type":1,"text":"Cons:","markups":[]},{"name":"d9cf","type":9,"text":"The trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). This impairs the interpretability of the model.","markups":[]},{"name":"2619","type":9,"text":"Separating border built by a decision tree has its limitations — it consists of hyperplanes perpendicular to one of the coordinate axes, which is inferior in quality to some other methods, in practice.","markups":[]},{"name":"5f2d","type":9,"text":"We need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. Note that overfitting is an issue for all machine learning methods.","markups":[]},{"name":"8e4f","type":9,"text":"Instability. Small changes to the data can significantly change the decision tree. This problem is tackled with decision tree ensembles (discussed next time).","markups":[]},{"name":"db79","type":9,"text":"The optimal decision tree search problem is NP-complete. Some heuristics are used in practice such as greedy search for a feature with maximum information gain, but it does not guarantee finding the globally optimal tree.","markups":[]},{"name":"7366","type":9,"text":"Difficulties to support missing values in the data. Friedman estimated that it took about 50% of the code to support gaps in data in CART (an improved version of this algorithm is implemented in sklearn).","markups":[{"type":10,"start":195,"end":202}]},{"name":"aa78","type":9,"text":"The model can only interpolate but not extrapolate (the same is true for random forests and tree boosting). That is, a decision tree makes constant prediction for the objects that lie beyond the bounding box set by the training set in the feature space. In our example with the yellow and blue balls, it would mean that the model gives the same predictions for all balls with positions \x3e19 or \x3c0.","markups":[]},{"name":"8193","type":13,"text":"Pros and cons of the nearest neighbors method","markups":[]},{"name":"2509","type":1,"text":"Pros:","markups":[]},{"name":"136e","type":9,"text":"Simple implementation.","markups":[]},{"name":"1c8b","type":9,"text":"Well studied.","markups":[]},{"name":"e7e2","type":9,"text":"Typically, the method is a good first solution not only for classification or regression, but also recommendations.","markups":[]},{"name":"e38e","type":9,"text":"It can be adapted to a certain problem by choosing the right metrics or kernel (in a nutshell, the kernel may set the similarity operation for complex objects such as graphs while keeping the k-NN approach the same). By the way, Alexander Dyakonov, a former top-1 kaggler, loves the simplest k-NN but with the tuned object similarity metric.","markups":[{"type":3,"start":229,"end":247,"href":"https://www.kaggle.com/dyakonov","title":"","rel":"noopener","anchorType":0}]},{"name":"69f7","type":9,"text":"Good interpretability. There are exceptions: if the number of neighbors is large, the interpretability deteriorates (“We did not give him a loan, because he is similar to the 350 clients, of which 70 are the bad, and that is 12% higher than the average for the dataset”).","markups":[]},{"name":"8bd5","type":1,"text":"Cons:","markups":[]},{"name":"fb3f","type":9,"text":"Method considered fast in comparison with compositions of algorithms, but the number of neighbors used for classification is usually large (100–150) in real life, in which case the algorithm will not operate as fast as a decision tree.","markups":[]},{"name":"c43d","type":9,"text":"If a dataset has many variables, it is difficult to find the right weights and to determine which features are not important for classification/regression.","markups":[]},{"name":"d99c","type":9,"text":"Dependency on the selected distance metric between the objects. Selecting the Euclidean distance by default is often unfounded. You can find a good solution by grid searching over parameters, but this becomes very time consuming for large datasets.","markups":[]},{"name":"2e36","type":9,"text":"There are no theoretical ways to choose the number of neighbors — only grid search (though this is often true for all hyperparameters of all models). In the case of a small number of neighbors, the method is sensitive to outliers, that is, it is inclined to overfit.","markups":[]},{"name":"7b5e","type":9,"text":"As a rule, it does not work well when there are a lot of features due to the “curse of dimensionality”. Professor Pedro Domingos, a well-known member in the ML community, talks about it here in his popular paper, “A Few Useful Things to Know about Machine Learning”; also “the curse of dimensionality” is described in the Deep Learning book in this chapter.","markups":[{"type":3,"start":186,"end":190,"href":"https://homes.cs.washington.edu/%7Epedrod/papers/cacm12.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":344,"end":356,"href":"http://www.deeplearningbook.org/contents/ml.html","title":"","rel":"","anchorType":0}]},{"name":"9ece","type":1,"text":"This is a lot of information, but, hopefully, this article will be a great reference for you for a long time :)","markups":[]},{"name":"e089","type":3,"text":"7. Assignment #3","markups":[]},{"name":"9ea4","type":1,"text":"A demo version is to appear soon (~ in the end of June 2018). Full version will be available in the new session of the course (starting on October, 1).","markups":[]},{"name":"93a3","type":3,"text":"8. Useful resources","markups":[]},{"name":"f545","type":9,"text":"Decision trees and k Nearest Neighbors are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).","markups":[]},{"name":"ccb1","type":9,"text":"The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.","markups":[]},{"name":"4ad3","type":9,"text":"Scikit-learn library. These guys work hard on writing really clear documentation.","markups":[{"type":3,"start":0,"end":12,"href":"http://scikit-learn.org/stable/documentation.html","title":"","rel":"noopener","anchorType":0}]},{"name":"f98d","type":9,"text":"Scipy 2017 scikit-learn tutorial by Alex Gramfort and Andreas Mueller.","markups":[{"type":3,"start":11,"end":32,"href":"https://github.com/amueller/scipy-2017-sklearn","title":"","rel":"noopener","anchorType":0}]},{"name":"b75c","type":9,"text":"One more ML course with very good materials.","markups":[{"type":3,"start":9,"end":18,"href":"https://github.com/diefimov/MTH594_MachineLearning","title":"","rel":"noopener","anchorType":0}]},{"name":"f7a6","type":9,"text":"Implementations of many ML algorithms. Good to search for decision trees and k-NN.","markups":[{"type":3,"start":0,"end":15,"href":"https://github.com/rushter/MLAlgorithms","title":"","rel":"noopener","anchorType":0}]},{"name":"bef3","type":9,"text":"And many others, feel free to share in comments.","markups":[]},{"name":"541b","type":1,"text":"Author: Yury Kashnitsky, Data Scientist at Mail.Ru Group. Translated and edited by Christina Butsko, Gleb Filatov, and Yuanyuan Pao.","markups":[{"type":3,"start":8,"end":23,"href":"https://www.linkedin.com/in/festline/","title":"","rel":"noopener nofollow noopener","anchorType":0},{"type":3,"start":83,"end":99,"href":"https://www.linkedin.com/in/christinabutsko/","title":"","rel":"nofollow noopener nofollow noopener nofollow noopener","anchorType":0},{"type":3,"start":119,"end":131,"href":"https://www.linkedin.com/in/yuanyuanpao/","title":"","rel":"noopener nofollow noopener","anchorType":0},{"type":2,"start":0,"end":132}]}],"sections":[{"name":"47bc","startIndex":0},{"name":"a08a","startIndex":287}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"1*5Tt3NDpNobfU_YeKglBRyQ.jpeg","filter":"","backgroundSize":"","originalWidth":1031,"originalHeight":775,"strategy":"resample","height":0,"width":0},"wordCount":6534,"imageCount":35,"readingTime":27.156603773584905,"subtitle":"Hi! This is the third article in our series. Today we finally reach machine learning. This is going to be exciting!","userPostRelation":{"userId":"a988255767e5","postId":"8613c6b6d2cd","readAt":1525188604710,"readLaterAddedAt":0,"votedAt":0,"collaboratorAddedAt":0,"notesAddedAt":0,"subscribedAt":0,"lastReadSectionName":"a08a","lastReadVersionId":"ae79dd03224","lastReadAt":1525188439234,"lastReadParagraphName":"541b","lastReadPercentage":1,"viewedAt":1525188429677,"presentedCountInResponseManagement":0,"clapCount":0,"seriesUpdateNotifsOptedInAt":0,"queuedAt":0,"seriesFirstViewedAt":0,"presentedCountInStream":17,"seriesLastViewedAt":0,"audioProgressSec":0},"publishedInCount":1,"usersBySocialRecommends":[],"recommends":186,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":34446,"virtuals":{"isFollowing":false},"metadata":{"followerCount":23747,"postCount":34446,"coverImage":{"id":"1*Rkd4uOPkU446v_QbcY-3lQ.jpeg","originalWidth":1011,"originalHeight":628}},"type":"Tag"},{"slug":"classification","name":"Classification","postCount":308,"virtuals":{"isFollowing":false},"metadata":{"followerCount":33,"postCount":308,"coverImage":{"id":"1*3oeNW3msP9qgNJfDrHZDiw.jpeg","originalWidth":1280,"originalHeight":737,"isFeatured":true}},"type":"Tag"},{"slug":"decision-tree","name":"Decision Tree","postCount":127,"virtuals":{"isFollowing":false},"metadata":{"followerCount":23,"postCount":127,"coverImage":{"id":"1*LPNd_EuOb7tLmMWpsuATDw.png","originalWidth":560,"originalHeight":397}},"type":"Tag"},{"slug":"scikit-learn","name":"Scikit Learn","postCount":167,"virtuals":{"isFollowing":false},"metadata":{"followerCount":51,"postCount":167,"coverImage":{"id":"1*mdPG5HAPKDxL1J-QMxTDKg.jpeg","originalWidth":1920,"originalHeight":1280}},"type":"Tag"},{"slug":"education","name":"Education","postCount":172642,"virtuals":{"isFollowing":false},"metadata":{"followerCount":636060,"postCount":172642,"coverImage":{"id":"1*dVmAyTviHMkGJq46uvrLcg.jpeg"}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":2,"links":{"entries":[{"url":"https://www.linkedin.com/in/yuanyuanpao/","alts":[],"httpStatus":999},{"url":"https://github.com/rushter/MLAlgorithms","alts":[],"httpStatus":200},{"url":"https://github.com/diefimov/MTH594_MachineLearning","alts":[],"httpStatus":200},{"url":"https://www.linkedin.com/in/festline/","alts":[],"httpStatus":999},{"url":"https://www.linkedin.com/in/christinabutsko/","alts":[],"httpStatus":999},{"url":"https://github.com/amueller/scipy-2017-sklearn","alts":[],"httpStatus":200},{"url":"http://scikit-learn.org/stable/documentation.html","alts":[],"httpStatus":200},{"url":"https://en.wikipedia.org/wiki/Curse_of_dimensionality","alts":[],"httpStatus":200},{"url":"http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html","alts":[],"httpStatus":200},{"url":"http://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic03_decision_trees_kNN/topic3_decision_trees_kNN.ipynb?flush_cache=true","alts":[],"httpStatus":200},{"url":"http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/","alts":[{"type":1,"url":"https://cdn.ampproject.org/c/kevinmeurer.com:80/a-simple-guide-to-entropy-based-discretization/amp/"}],"httpStatus":200},{"url":"http://www.deeplearningbook.org/","alts":[],"httpStatus":200},{"url":"https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220","alts":[{"type":2,"url":"medium://p/44a41b9b5220"},{"type":3,"url":"medium://p/44a41b9b5220"}],"httpStatus":200},{"url":"https://github.com/spotify/annoy","alts":[],"httpStatus":200},{"url":"http://www.deeplearningbook.org/contents/ml.html","alts":[],"httpStatus":200},{"url":"https://github.com/Yorko/mlcourse_open","alts":[],"httpStatus":200},{"url":"http://www.cs.uvm.edu/%7Eicdm/algorithms/10Algorithms-08.pdf","alts":[],"httpStatus":200},{"url":"https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html","alts":[],"httpStatus":200},{"url":"http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx","alts":[],"httpStatus":200},{"url":"https://homes.cs.washington.edu/%7Epedrod/papers/cacm12.pdf","alts":[],"httpStatus":200},{"url":"https://habrahabr.ru/post/171759/","alts":[{"type":3,"url":"habrahabr://post/171759"}],"httpStatus":200},{"url":"https://alvenka.deviantart.com/art/fractal-tree-34-366989431","alts":[],"httpStatus":200},{"url":"https://www.kaggle.com/dyakonov","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1525725827734},"isLockedPreviewOnly":false,"takeoverId":"","metaDescription":"","totalClapCount":3844,"sectionCount":2,"readingList":0},"coverless":true,"slug":"open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"https://gist.github.com/Yorko/f277786fab129d97a22a9ce795bf6fe6","importedPublishedAt":0,"visibility":0,"uniqueSlug":"open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd","previewContent":{"bodyModel":{"paragraphs":[{"name":"previewImage","type":4,"text":"","layout":10,"metadata":{"id":"1*5Tt3NDpNobfU_YeKglBRyQ.jpeg","originalWidth":1031,"originalHeight":775,"isFeatured":true}},{"name":"7b0c","type":3,"text":"Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors","markups":[],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd","approvedHomeCollectionId":"b76595fb6cfc","approvedHomeCollection":{"id":"b76595fb6cfc","name":"Open Machine Learning Course","slug":"open-machine-learning-course","tags":["MACHINE LEARNING","DATA SCIENCE","DATA ANALYSIS","EDUCATION","KAGGLE"],"creatorId":"5572ec600139","description":"A series of articles on basics of Machine Learning. Each article is followed by an assignment with a deadline. Several Kaggle Inclass competitions are held throughout the course.","shortDescription":"A series of articles on basics of Machine Learning.","image":{"imageId":"1*b5pfoCQz20rjh79OPSQWQg.jpeg","filter":"","backgroundSize":"","originalWidth":271,"originalHeight":271,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":1394,"activeAt":1523869737051},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false},"isSubscribed":true,"isNewsletterSubscribed":true,"memberOfMembershipPlanId":"","isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"1*MxsqShnPs3RXMScwl5HhsQ.jpeg","filter":"","backgroundSize":"","originalWidth":1479,"originalHeight":822,"strategy":"resample","height":0,"width":0},"twitterUsername":"odsai_en","facebookPageName":"DataChallenges","publicEmail":"yury.kashnitsky@gmail.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"Open Machine Learning Course","description":"by OpenDataScience","backgroundImage":{"id":"1*ZnqNzhbOlqb19tLgWDn8ww.png","originalWidth":2400,"originalHeight":1200},"logoImage":{},"alignment":2,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":6,"number":10,"postIds":[]}}],"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"Open Machine Learning Course","description":"by OpenDataScience","backgroundImage":{"id":"1*ZnqNzhbOlqb19tLgWDn8ww.png","originalWidth":2400,"originalHeight":1200},"logoImage":{},"alignment":2,"layout":6},"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://gist.github.com/Yorko/f277786fab129d97a22a9ce795bf6fe6","mediumUrl":"https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"isSponsored":false,"isRequestToPubDisabled":true,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"featureLockRequestMinimumGuaranteeAmount":0,"isElevate":false,"mongerRequestType":1,"type":"Post"},"mentionedUsers":[],"collaborators":[],"membershipPlans":[],"collectionUserRelations":[],"mode":null,"references":{"User":{"5572ec600139":{"userId":"5572ec600139","name":"Yury Kashnitskiy","username":"yurykashnitskiy","createdAt":1490200873092,"lastPostCreatedAt":1525724892687,"imageId":"0*JtleyoApNKIjJU5F.","backgroundImageId":"","bio":"Data Scientist at Mail.Ru Group","twitterScreenName":"","socialStats":{"userId":"5572ec600139","usersFollowedCount":33,"usersFollowedByCount":650,"type":"SocialStats"},"social":{"userId":"a988255767e5","targetUserId":"5572ec600139","type":"Social"},"facebookAccountId":"1414603208563160","allowNotes":1,"isNsfw":false,"type":"User"}},"Collection":{"b76595fb6cfc":{"id":"b76595fb6cfc","name":"Open Machine Learning Course","slug":"open-machine-learning-course","tags":["MACHINE LEARNING","DATA SCIENCE","DATA ANALYSIS","EDUCATION","KAGGLE"],"creatorId":"5572ec600139","description":"A series of articles on basics of Machine Learning. Each article is followed by an assignment with a deadline. Several Kaggle Inclass competitions are held throughout the course.","shortDescription":"A series of articles on basics of Machine Learning.","image":{"imageId":"1*b5pfoCQz20rjh79OPSQWQg.jpeg","filter":"","backgroundSize":"","originalWidth":271,"originalHeight":271,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":1394,"activeAt":1523869737051},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false},"isSubscribed":true,"isNewsletterSubscribed":true,"memberOfMembershipPlanId":"","isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"1*MxsqShnPs3RXMScwl5HhsQ.jpeg","filter":"","backgroundSize":"","originalWidth":1479,"originalHeight":822,"strategy":"resample","height":0,"width":0},"twitterUsername":"odsai_en","facebookPageName":"DataChallenges","publicEmail":"yury.kashnitsky@gmail.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"Open Machine Learning Course","description":"by OpenDataScience","backgroundImage":{"id":"1*ZnqNzhbOlqb19tLgWDn8ww.png","originalWidth":2400,"originalHeight":1200},"logoImage":{},"alignment":2,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":6,"number":10,"postIds":[]}}],"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF02B875","point":0},{"color":"#FF00AB6B","point":0.1},{"color":"#FF1C9963","point":0.2},{"color":"#FF092E20","point":1}],"backgroundColor":"#FFFFFFFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFFFFFFF","point":0},{"color":"#FFE9FDF0","point":0.1},{"color":"#FFE2FAEE","point":0.2},{"color":"#FFADFFCF","point":0.6},{"color":"#FF7DFFB3","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"Open Machine Learning Course","description":"by OpenDataScience","backgroundImage":{"id":"1*ZnqNzhbOlqb19tLgWDn8ww.png","originalWidth":2400,"originalHeight":1200},"logoImage":{},"alignment":2,"layout":6},"type":"Collection"}},"Social":{"5572ec600139":{"userId":"a988255767e5","targetUserId":"5572ec600139","type":"Social"}},"SocialStats":{"5572ec600139":{"userId":"5572ec600139","usersFollowedCount":33,"usersFollowedByCount":650,"type":"SocialStats"}}}})
// ]]></script><div class="surface-scrollOverlay"></div><script charset="UTF-8" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/main-common-async.bundle.S7356n6LBseGGEx0Avxzng.js"></script><script charset="UTF-8" src="./Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors_files/main-notes.bundle.CzX6ttriZ_q3KYPaiNXuzw.js"></script></body></html>